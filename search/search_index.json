{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The R\u0101poi HPC Cluster (aka raapoi) is a University-wide computing resource that uses the Slurm resource manager to schedule jobs and reserve resources. Similar to most modern compute clusters, R\u0101poi requires you to request CPU, Memory and Time for your job. If you do not request these resources, you will be given the minimal defaults, which may not be enough to run your job. The good news about resource reservations is that the resources you request are guaranteed to be yours, the bad news is if you request too little memory or time, your job may terminate prematurely and if you request too few CPUs then your job may run slowly. R\u0101poi is made up of partitions. A partition is a set of compute nodes (servers) and each partition has its own configuration and hardware profile. The partition on which you run your jobs will depend on the type of workflow or job you intend to submit. The cluster employs the module environment to allow researchers to customise their environment with their required applications and languages. The documentation contained in this wiki cover most of what you will need to know to start running jobs, and if you need more help, please see the training tutorials with step-by-step instructions for our most popular apps and languages. You'll also find some simpler examples on this site. Something in this documentation not up to date or incorrect? Feel free to create an issue on the documentation github page . Better yet, submit a pull request :) Current R\u0101poi hardware layout","title":"Overview"},{"location":"accessing_the_cluster/","text":"Accessing the Cluster \u00b6 To access R\u0101poi, you'll first need to get an account provisioned for you by contacting the CAD research support team with your: Full Name VUW staff username Faculty, School or Institute affiliation. If you don't have a VUW staff account, it may still be possible to be given access - please contact us to determine options. Access is via SSH Hostname: raapoi.vuw.ac.nz IP Address: 130.195.19.14 Port: 22 Username: Your VUW username Password: Your VUW password NOTE: A wired network connection or VPN is required if connecting from campus wifi or from off-campus. Some users have had issues with using the hostname and instead need to use the IP address, eg harrelwe@130.195.19.14 More information on VUW VPN services can be found here . Here is a general overview of SSH https://www.howtogeek.com/311287/how-to-connect-to-an-ssh-server-from-windows-macos-or-linux/ . SSH Clients \u00b6 Mac OSX SSH Clients You can use the built-in Terminal.app or you can download iTerm2 or XQuartz. XQuartz is required to be installed if you wish to forward GUI applications (matlab, rstudio, xstata, sas, etc), aka X forwarding. Terminal.app is the default application for command-line interface To login using the built-in Terminal.app on Mac, go to Applications --> Utilities --> Terminal.app Or use Spotlight search (aka Command-Space) iTerm2 is a good replacement for the default Terminal app XQuartz is a Xforwarding application with its own terminal. XQuartz can be used in conjuction with the Terminal.app for GUI apps. NOTE: Mac users should run the following command: sudo defaults write org.macosforge.xquartz.X11 enable_iglx -bool true We have found that this allows some older GUI applications to run with fewer errors. NOTE: Once at the command prompt you can type the following to login (replace \"username\" with your VUW user): ssh -X username@raapoi.vuw.ac.nz The -X parameter tells SSH to forward any GUI windows to your local machine, this is called X forwarding. Windows SSH Clients Recommended Clients: Git Bash is a great option and is part of the Git for Windows project. MobaXterm is a good option, especially if you require access to GUI applications such as MATLAB or xStata. This also has a built-in SFTP transfer window. File Transfer with SFTP, SCP or rsync \u00b6 There are many file transfer clients available for Mac, Windows and Linux, including but not limited to Free/OpenSource Desktop tools such as Filezilla, Cyberduck, Dolphin and proprietary/licenced offerings such as WinSCP, ExpanDrive, etc One can also use built-in command-line tools on Linux, Mac and Windows (if running Git Bash or MobaXterm). The most common command-line utilities are scp, sftp and rsync In all cases you will need to supply the hostname or IP address of the cluster, see above. You may also need to supply the port (22) and a path. The paths that you will most likely use are your home or your scratch space: /nfs/home/username or /nfs/scratch/username File transfer with cloud tools \u00b6 If you are using cloud storage such as AWS, DropBox, Cloudstor please look at the examples we have in Connecting to Cloud Providers","title":"Accessing the Cluster"},{"location":"accessing_the_cluster/#accessing-the-cluster","text":"To access R\u0101poi, you'll first need to get an account provisioned for you by contacting the CAD research support team with your: Full Name VUW staff username Faculty, School or Institute affiliation. If you don't have a VUW staff account, it may still be possible to be given access - please contact us to determine options. Access is via SSH Hostname: raapoi.vuw.ac.nz IP Address: 130.195.19.14 Port: 22 Username: Your VUW username Password: Your VUW password NOTE: A wired network connection or VPN is required if connecting from campus wifi or from off-campus. Some users have had issues with using the hostname and instead need to use the IP address, eg harrelwe@130.195.19.14 More information on VUW VPN services can be found here . Here is a general overview of SSH https://www.howtogeek.com/311287/how-to-connect-to-an-ssh-server-from-windows-macos-or-linux/ .","title":"Accessing the Cluster"},{"location":"accessing_the_cluster/#ssh-clients","text":"Mac OSX SSH Clients You can use the built-in Terminal.app or you can download iTerm2 or XQuartz. XQuartz is required to be installed if you wish to forward GUI applications (matlab, rstudio, xstata, sas, etc), aka X forwarding. Terminal.app is the default application for command-line interface To login using the built-in Terminal.app on Mac, go to Applications --> Utilities --> Terminal.app Or use Spotlight search (aka Command-Space) iTerm2 is a good replacement for the default Terminal app XQuartz is a Xforwarding application with its own terminal. XQuartz can be used in conjuction with the Terminal.app for GUI apps. NOTE: Mac users should run the following command: sudo defaults write org.macosforge.xquartz.X11 enable_iglx -bool true We have found that this allows some older GUI applications to run with fewer errors. NOTE: Once at the command prompt you can type the following to login (replace \"username\" with your VUW user): ssh -X username@raapoi.vuw.ac.nz The -X parameter tells SSH to forward any GUI windows to your local machine, this is called X forwarding. Windows SSH Clients Recommended Clients: Git Bash is a great option and is part of the Git for Windows project. MobaXterm is a good option, especially if you require access to GUI applications such as MATLAB or xStata. This also has a built-in SFTP transfer window.","title":"SSH Clients"},{"location":"accessing_the_cluster/#file-transfer-with-sftp-scp-or-rsync","text":"There are many file transfer clients available for Mac, Windows and Linux, including but not limited to Free/OpenSource Desktop tools such as Filezilla, Cyberduck, Dolphin and proprietary/licenced offerings such as WinSCP, ExpanDrive, etc One can also use built-in command-line tools on Linux, Mac and Windows (if running Git Bash or MobaXterm). The most common command-line utilities are scp, sftp and rsync In all cases you will need to supply the hostname or IP address of the cluster, see above. You may also need to supply the port (22) and a path. The paths that you will most likely use are your home or your scratch space: /nfs/home/username or /nfs/scratch/username","title":"File Transfer with SFTP, SCP or rsync"},{"location":"accessing_the_cluster/#file-transfer-with-cloud-tools","text":"If you are using cloud storage such as AWS, DropBox, Cloudstor please look at the examples we have in Connecting to Cloud Providers","title":"File transfer with cloud tools"},{"location":"basic_commands/","text":"Basic Commands \u00b6 The vuw Commands \u00b6 In an effort to make using R\u0101poi just a bit easier, CAD staff have created commands to help you view useful information. We call these the vuw commands. This is because all the commands begin with the string vuw . This makes it easier to see the commands available to you. If, at a command prompt you type vuw followed immediately by two TAB keys you will see a list of available commands beginning with vuw . Go ahead and type vuw-TAB-TAB to see for yourself. The commands available as of this update are: vuw-help : Prints this help information vuw-job-report : Provides some summary information about a job vuw-quota : Prints current storage quota and usage vuw-partitions : Prints a list of available partitions and the availability of compute nodes vuw-alljobs : Prints a list of all user jobs vuw-myjobs : Prints a list of your running or pending jobs vuw-job-history : Show jobs finished in last 5 days vuw-job-eff : Show efficiency of your jobs. Use vuw-job-eff --help for more information Linux Commands \u00b6 R\u0101poi is built using the Linux operating system. Access is primarily via command line interface (CLI) as opposed to the graphical user interfaces (GUI) that you are more familiar with (such as those on Windows or Mac) Below are a list of common commands for viewing and managing files and directories (replace the file and directory names with ones you own): ls - This command lists the contents of the current directory * ls -l This is the same command with a flag (-l) which lists the contents with more information, including access permissions * ls -a Same ls command but this time the -a flag which will also list hidden files. Hidden files start with a . (period) * ls -la Stringing flags together cd - This will change your location to a different directory (folder) * cd projects/calctest_proj * Typing cd with no arguments will take you back to your home directory mv - This will move or rename a file * mv project1.txt project2.txt * mv project2.txt projects/calctest_proj/ cp - This allows you to copy file/s and/or directories to defined locations. The cp command works very much like mv , except it copies a file instead of moving it. The general form of the command is cp source destination , for example: cp myfile.txt myfilecopy.txt Further examples and options can be seen here . rm - This will delete a file * rm projects/calctest_proj/projects2.txt * rm -r projects/calctest_proj/code The -r flag recursively removes files and directories mkdir - This will create a new directory * mkdir /nfs/home/myusername/financial To find more detailed information about any command you can use the manpages, eg: man ls Learning the Linux Shell \u00b6 A good tutorial for using linux can be found here: Learning the linux shell . Software Carpentry also provides a good introduction to the shell, including how to work with files and directories .","title":"Basic Commands"},{"location":"basic_commands/#basic-commands","text":"","title":"Basic Commands"},{"location":"basic_commands/#the-vuw-commands","text":"In an effort to make using R\u0101poi just a bit easier, CAD staff have created commands to help you view useful information. We call these the vuw commands. This is because all the commands begin with the string vuw . This makes it easier to see the commands available to you. If, at a command prompt you type vuw followed immediately by two TAB keys you will see a list of available commands beginning with vuw . Go ahead and type vuw-TAB-TAB to see for yourself. The commands available as of this update are: vuw-help : Prints this help information vuw-job-report : Provides some summary information about a job vuw-quota : Prints current storage quota and usage vuw-partitions : Prints a list of available partitions and the availability of compute nodes vuw-alljobs : Prints a list of all user jobs vuw-myjobs : Prints a list of your running or pending jobs vuw-job-history : Show jobs finished in last 5 days vuw-job-eff : Show efficiency of your jobs. Use vuw-job-eff --help for more information","title":"The vuw Commands"},{"location":"basic_commands/#linux-commands","text":"R\u0101poi is built using the Linux operating system. Access is primarily via command line interface (CLI) as opposed to the graphical user interfaces (GUI) that you are more familiar with (such as those on Windows or Mac) Below are a list of common commands for viewing and managing files and directories (replace the file and directory names with ones you own): ls - This command lists the contents of the current directory * ls -l This is the same command with a flag (-l) which lists the contents with more information, including access permissions * ls -a Same ls command but this time the -a flag which will also list hidden files. Hidden files start with a . (period) * ls -la Stringing flags together cd - This will change your location to a different directory (folder) * cd projects/calctest_proj * Typing cd with no arguments will take you back to your home directory mv - This will move or rename a file * mv project1.txt project2.txt * mv project2.txt projects/calctest_proj/ cp - This allows you to copy file/s and/or directories to defined locations. The cp command works very much like mv , except it copies a file instead of moving it. The general form of the command is cp source destination , for example: cp myfile.txt myfilecopy.txt Further examples and options can be seen here . rm - This will delete a file * rm projects/calctest_proj/projects2.txt * rm -r projects/calctest_proj/code The -r flag recursively removes files and directories mkdir - This will create a new directory * mkdir /nfs/home/myusername/financial To find more detailed information about any command you can use the manpages, eg: man ls","title":"Linux Commands"},{"location":"basic_commands/#learning-the-linux-shell","text":"A good tutorial for using linux can be found here: Learning the linux shell . Software Carpentry also provides a good introduction to the shell, including how to work with files and directories .","title":"Learning the Linux Shell"},{"location":"cloud_providers/","text":"Connecting to Cloud Providers \u00b6 AARNET Cloudstor \u00b6 All VUW researchers have access to the AARNET (Australia\u2019s Academic and Research Network) Cloudstor service which provides 1 TB of space to each researcher. To use this service first login and download an appropriate client to your laptop or desktop (or smarthone if you wish): Cloudstor Login NOTE: Within the Cloudstor web login settings you will need to create a Cloudstor Password, this is the password you will use to login on R\u0101poi, it does not use your VUW credentials for the command line login. We suggest setting up an App Password for Raapoi-rcopy rather than a global sync password. This way if your password is compromised you can easily just remove that app password. Setup an App Password by clicky on the settings gear on the top right and finding the App Password link . Once you have setup your cloudstor (aka ownCloud) credentials you can use them to sync data to and from R\u0101poi. For example, if I wanted to sync my project space to Cloudstor I would do the following from R\u0101poi login node: # Use Tmux to keep your session alive if you disconnect. You can reconnect to your Tmux session if you reconnect. See Tmux docs. tmux # Use our new module system module use /home/software/tools/eb_modulefiles/all/Core module load rclone/1.54.1 #check if cloudstor remote is already configured rclone listremotes The above sequence starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect) and then loads the rcopy module - which requires the use of our new module system. If you don't already have CloudStor configured as a remote (which you won't if this is your first time using it) follow the instructions on aarnet docs page . Once we have setup rclone to connect to CloudStor, we copy our data. In this case from <my scratch folder>/test to test on CloudStor rclone copy --progress --transfers 8 /nfs/scratch/geldenan/test CloudStor:/test Amazon AWS \u00b6 A feature-rich CLI is available in R\u0101poi. To use it you need to load the appropriate module and its module dependencies: module load amazon/aws/cli Before you proceed you will need to configure your environment with your Access Key ID and Secret Access Key, both of which will be sent to you once your account is created or linked. The command to configure your environment is aws configure You only need to do this once, unless of course you use more than one user/Access Key. Most users can simply click through the region and profile questions (using the default of \"none\"). If you do have a specific region this should be relayed along with your access and secret keys. Once you have the appropriate environment in place and your configuration setup you can use the aws command, plus an appropriate sub-command (s3, emr, rds, dynamodb, etc) and supporting arguments. More information on the CLI can be found here: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html Transferring Data to/from Amazon (AWS) S3 \u00b6 To transfer data from S3 you first need to setup your AWS connect, instructions for that can be found above. Once that is done you should be able to use the aws commands to copy data to and from your S3 storage. For example if I wanted to copy data from my S3 storage to my project directory I could do the following: tmux module load amazon/aws/cli cd /nfs/scratch/harrelwe/project aws s3 cp s3://mybucket/mydata.dat mydata.dat To copy something to storage simply reverse the file paths, eg. aws s3 cp mydata.dat s3://mybucket/mydata.dat The above starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect). I then load the modules necessary to use the AWS commands. I change directory to my project space and use the aws s3 cp command to copy from S3. More information on using aws can be found here: http://docs.aws.amazon.com/cli/latest/reference/s3/index.html#cli-aws-s3 Working with AWS Data Analysis Tools \u00b6 Amazon has a number of data analytics and database services available. Using the command line utilities available in R\u0101poi, researchers can perform work on the eo cluster and transfer data to AWS to perform further analysis with tools such as MapReduce (aka Hadoop), RedShift or Quicksight. A listing of available services and documentation can be found at the following: https://aws.amazon.com/products/analytics/ Google Cloud (gcloud) Connections \u00b6 The Google Cloud SDK is available in R\u0101poi. This includes a command-line interface for connecting to gloud services. To get started, first load the environment module. You can find the path with the module spider command. As of this writing the current version can be loaded thusly: module load google/cloud/sdk/212.0.0 This will give you access to the gcloud command. To setup a connection to your gcloud account use the init sub-command, eg. gcloud init --console-only Follow the instructions to authorize your gcloud account. Once on the Google website, you will be given an authorization code which you will copy/paste back into the R\u0101poi terminal window. Transferring Data to/from Google Cloud (gcloud) \u00b6 To transfer data from gcloud storage you first need to setup your gcloud credentials, instructions for that can be found above. Once that is done you should be able to use the gsutil command to copy data to and from your gcloud storage. For example, if I wanted to copy data from gcloud to my project directory I could do the following: tmux module load google/cloud/sdk/212.0.0 cd /nfs/scratch/harrelwe/project gsutil cp gs://mybucket/mydata.dat mydata.dat To copy something to storage simply reverse the file paths, eg. gsutil cp mydata.dat gs://mybucket/mydata.dat The above starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect). I then load the modules necessary to use the gsutil commands. I change directory to my project space and use the gsutil cp command to copy from gcloud. More information on using gcloud can be found here: https://cloud.google.com/sdk/gcloud/ Working with GCloud Data Analysis Tools \u00b6 Google Cloud has a number of data analytics and database services available. Using the gcloud command line utilities available on R\u0101poi, researchers can perform work on the cluster and transfer data to gcloud to perform further analysis with tools such as Dataproc (Hadoop/Spark), BigQuery or Datalab (Visualization) A listing of available services and documentation can be found at the following: https://cloud.google.com/products/ DropBox Cloud Storage \u00b6 NOTE: Dropbox has upload/download limitations and we have found that once your file gets above 50GB in size the transfer will have a better chance of timing out and failing. Configuring your Dropbox account on R\u0101poi Step A: On your local laptop or desktop start your browser and login to your Dropbox account Step B: On R\u0101poi type the following: module load dropbox Step C: Setup account credentials (You should only need to do this once): Run the following command from R\u0101poi dbxcli account You will now see something like the following: Go to https://www.dropbox.com/1/oauth2/authorize?client_id=X12345678&response_type=code&state=state Click \"Allow\" (you might have to log in first). Copy the authorization code. Enter the authorization code here: Step D: Copy the URL link listed in Step C1 and paste it into the web browser that you started in Step A This will provide you with a long access code (aka hash). Now copy that access code and paste it into your R\u0101poi terminal after Step C3 where it is asking for Enter the authorization code here Now hit enter or return. You should see that you are now logged in with your Dropbox credentials Basic Dropbox commands \u00b6 Remember to load the dropbox environment module if you have not already (see module spider for the path) Now type dbx or dbxcli at a prompt. You will see a number of sub-commands, for instance ls, which will list the contents of your Dropbox, eg dbxcli ls Downloading from Dropbox \u00b6 Downloading uses the subcommand called: get. The basic format for get is: dbxcli get fileOnDropbox fileOnRaapoi For instance, if I have a datafile called 2018-financials.csv on Dropbox that I want to copy to my project folder I would type: dbxcli get 2018-financials.csv /nfs/scratch/harrelwe/projects/finance_proj/2018-financials.csv Uploading to Dropbox \u00b6 Uploading is similar to downloading except now we use the subcommand: put. The basic format for put is: dbxcli put fileOnRaapoi fileOnDropbox For example I want to upload a PDF I generated from one of my jobs called final-report.pdf I would type: dbxcli put final-report.pdf final-report.pdf This will upload the PDF and name it the same thing, if I wanted to change the name on Dropbox I could: dbxcli put final-report.pdf analytics-class-final-report.pdf","title":"Connecting to Cloud Providers"},{"location":"cloud_providers/#connecting-to-cloud-providers","text":"","title":"Connecting to Cloud Providers"},{"location":"cloud_providers/#aarnet-cloudstor","text":"All VUW researchers have access to the AARNET (Australia\u2019s Academic and Research Network) Cloudstor service which provides 1 TB of space to each researcher. To use this service first login and download an appropriate client to your laptop or desktop (or smarthone if you wish): Cloudstor Login NOTE: Within the Cloudstor web login settings you will need to create a Cloudstor Password, this is the password you will use to login on R\u0101poi, it does not use your VUW credentials for the command line login. We suggest setting up an App Password for Raapoi-rcopy rather than a global sync password. This way if your password is compromised you can easily just remove that app password. Setup an App Password by clicky on the settings gear on the top right and finding the App Password link . Once you have setup your cloudstor (aka ownCloud) credentials you can use them to sync data to and from R\u0101poi. For example, if I wanted to sync my project space to Cloudstor I would do the following from R\u0101poi login node: # Use Tmux to keep your session alive if you disconnect. You can reconnect to your Tmux session if you reconnect. See Tmux docs. tmux # Use our new module system module use /home/software/tools/eb_modulefiles/all/Core module load rclone/1.54.1 #check if cloudstor remote is already configured rclone listremotes The above sequence starts a tmux session to allow the transfer to continue even if I disconnect from the cluster (type tmux attach to reconnect) and then loads the rcopy module - which requires the use of our new module system. If you don't already have CloudStor configured as a remote (which you won't if this is your first time using it) follow the instructions on aarnet docs page . Once we have setup rclone to connect to CloudStor, we copy our data. In this case from <my scratch folder>/test to test on CloudStor rclone copy --progress --transfers 8 /nfs/scratch/geldenan/test CloudStor:/test","title":"AARNET Cloudstor"},{"location":"cloud_providers/#amazon-aws","text":"A feature-rich CLI is available in R\u0101poi. To use it you need to load the appropriate module and its module dependencies: module load amazon/aws/cli Before you proceed you will need to configure your environment with your Access Key ID and Secret Access Key, both of which will be sent to you once your account is created or linked. The command to configure your environment is aws configure You only need to do this once, unless of course you use more than one user/Access Key. Most users can simply click through the region and profile questions (using the default of \"none\"). If you do have a specific region this should be relayed along with your access and secret keys. Once you have the appropriate environment in place and your configuration setup you can use the aws command, plus an appropriate sub-command (s3, emr, rds, dynamodb, etc) and supporting arguments. More information on the CLI can be found here: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-using.html","title":"Amazon AWS"},{"location":"cloud_providers/#google-cloud-gcloud-connections","text":"The Google Cloud SDK is available in R\u0101poi. This includes a command-line interface for connecting to gloud services. To get started, first load the environment module. You can find the path with the module spider command. As of this writing the current version can be loaded thusly: module load google/cloud/sdk/212.0.0 This will give you access to the gcloud command. To setup a connection to your gcloud account use the init sub-command, eg. gcloud init --console-only Follow the instructions to authorize your gcloud account. Once on the Google website, you will be given an authorization code which you will copy/paste back into the R\u0101poi terminal window.","title":"Google Cloud (gcloud) Connections"},{"location":"cloud_providers/#dropbox-cloud-storage","text":"NOTE: Dropbox has upload/download limitations and we have found that once your file gets above 50GB in size the transfer will have a better chance of timing out and failing. Configuring your Dropbox account on R\u0101poi Step A: On your local laptop or desktop start your browser and login to your Dropbox account Step B: On R\u0101poi type the following: module load dropbox Step C: Setup account credentials (You should only need to do this once): Run the following command from R\u0101poi dbxcli account You will now see something like the following: Go to https://www.dropbox.com/1/oauth2/authorize?client_id=X12345678&response_type=code&state=state Click \"Allow\" (you might have to log in first). Copy the authorization code. Enter the authorization code here: Step D: Copy the URL link listed in Step C1 and paste it into the web browser that you started in Step A This will provide you with a long access code (aka hash). Now copy that access code and paste it into your R\u0101poi terminal after Step C3 where it is asking for Enter the authorization code here Now hit enter or return. You should see that you are now logged in with your Dropbox credentials","title":"DropBox Cloud Storage"},{"location":"containers/","text":"Using Containers \u00b6 Researchers can use Docker or Singularity containers within the cluster. This is a great way to run difficult-to-compile applications or to share workflows among colleagues. Running an interactive container \u00b6 User can run within a container interactively, this is great for testing code before running a job. Here is an example of running within a docker container that has the blockchain software called BlockSci: module load singularity srun --pty -c 4 --mem=16G bash singularity pull docker://tislaamo/blocksci singularity shell blocksci.simg Once you have typed the singularity shell command you will be within the container and can type the commands available from within the container such as the BlockSci utility blocksci_parser Running a container in batch \u00b6 Running a batch job with containers is similar to running a regular job, but will ultimately depend on how the container was created, so your mileage may vary. Here is an example batch submit script that will run the autometa software that was created in a docker image, lets name the submit file runContainer.sh: #SBATCH -J autometa-job #SBATCH -c 4 #SBATCH --mem=16G #SBATCH --mailtype=BEGIN,END,FAIL #SBATCH --mail-user=myemail@email.net #SBATCH --time=12:00:00 module load singularity singularity pull docker://jasonkwan/autometa:latest singularity exec autometa_latest.sif calculate_read_coverage.py somedata.dat Now to run the file you can: sbatch runContainer.sh Note that singularity shell is primarily for interactive use and singularity exec (or possibly singularity run ) are for executing the applications that were built within the container directly. It is important to know how the container was created to make effective use of the software.","title":"Using Containers"},{"location":"containers/#using-containers","text":"Researchers can use Docker or Singularity containers within the cluster. This is a great way to run difficult-to-compile applications or to share workflows among colleagues.","title":"Using Containers"},{"location":"containers/#running-an-interactive-container","text":"User can run within a container interactively, this is great for testing code before running a job. Here is an example of running within a docker container that has the blockchain software called BlockSci: module load singularity srun --pty -c 4 --mem=16G bash singularity pull docker://tislaamo/blocksci singularity shell blocksci.simg Once you have typed the singularity shell command you will be within the container and can type the commands available from within the container such as the BlockSci utility blocksci_parser","title":"Running an interactive container"},{"location":"containers/#running-a-container-in-batch","text":"Running a batch job with containers is similar to running a regular job, but will ultimately depend on how the container was created, so your mileage may vary. Here is an example batch submit script that will run the autometa software that was created in a docker image, lets name the submit file runContainer.sh: #SBATCH -J autometa-job #SBATCH -c 4 #SBATCH --mem=16G #SBATCH --mailtype=BEGIN,END,FAIL #SBATCH --mail-user=myemail@email.net #SBATCH --time=12:00:00 module load singularity singularity pull docker://jasonkwan/autometa:latest singularity exec autometa_latest.sif calculate_read_coverage.py somedata.dat Now to run the file you can: sbatch runContainer.sh Note that singularity shell is primarily for interactive use and singularity exec (or possibly singularity run ) are for executing the applications that were built within the container directly. It is important to know how the container was created to make effective use of the software.","title":"Running a container in batch"},{"location":"environment/","text":"Enviroment Setup \u00b6 Preparing your environment \u00b6 R\u0101poi has an extensive library of applications and software available. There are numerous programming languages and libraries (R, Julia, Python, lua, OpenMPI, blas, etc) as well as dozens of applications (Matlab, Gaussian, etc). We also keep older versions of software to ensure compatibility. Because of this, R\u0101poi developers use a tool called lmod to allow a user to load a specific version of an application, language or library and start using it for their work. The module command will show you what software is available to load, and will add the software to your environment for immediate use. To show all software available to load type the following: module avail You will see a long list of available modules to load, including a path, eg lua/5.3.5 However, instead of searching through a long list, if you know you want to use lua, you can find the path with the keyword subcommand: module keyword lua If you want to know more about a particular module you can use the whatis or show subcommand. Some modules have this available, for instance: harrelwe@raapoi-master:~$ module show trimmomatic/20190304 ---------------------------------------------------------------------- /home/software/tools/modulefiles/trimmomatic/20190304: ---------------------------------------------------------------------- load(\"java/jdk/1.8.0_121\") setenv(\"TM_HOME\",\"/home/software/apps/trimmomatic/20190304/bin\") module whatis R/CRAN/3.5 R/CRAN/3.5 : Adds the R library path to the pre-built CRAN modules Adding or loading software \u00b6 Once you have found the module path you can load the software: module load lua/5.3.5 After the module loads you can type srun --pty lua at a prompt, or add it to the path of your lua script (the RC team recommends using /usr/bin/env instead of an absolute path). Showing/listing the module environment modifications You can discover what the module will load into your environment you can run module show, for example here is what R adds: module show R/3.5.1 -------------------------------------------------- /home/software/tools/modulefiles/R/3.5.1: -------------------------------------------------- whatis(\"Adds the R language path to your environment \") prepend_path(\"PATH\",\"/home/software/apps/R/3.5.1/bin\") Listing loaded modules \u00b6 To see what modules you have loaded into your environment you can run the command: module list By default you will have the config module loaded (please do not unload that module). For example, here are the modules I have loaded in my environment when I wrote this section: module list Currently Loaded Modules: 1) config 2) tassel/3 3) python/3.7.0 4) python/modules/3.7","title":"Preparing your Environment (modules)"},{"location":"environment/#enviroment-setup","text":"","title":"Enviroment Setup"},{"location":"environment/#preparing-your-environment","text":"R\u0101poi has an extensive library of applications and software available. There are numerous programming languages and libraries (R, Julia, Python, lua, OpenMPI, blas, etc) as well as dozens of applications (Matlab, Gaussian, etc). We also keep older versions of software to ensure compatibility. Because of this, R\u0101poi developers use a tool called lmod to allow a user to load a specific version of an application, language or library and start using it for their work. The module command will show you what software is available to load, and will add the software to your environment for immediate use. To show all software available to load type the following: module avail You will see a long list of available modules to load, including a path, eg lua/5.3.5 However, instead of searching through a long list, if you know you want to use lua, you can find the path with the keyword subcommand: module keyword lua If you want to know more about a particular module you can use the whatis or show subcommand. Some modules have this available, for instance: harrelwe@raapoi-master:~$ module show trimmomatic/20190304 ---------------------------------------------------------------------- /home/software/tools/modulefiles/trimmomatic/20190304: ---------------------------------------------------------------------- load(\"java/jdk/1.8.0_121\") setenv(\"TM_HOME\",\"/home/software/apps/trimmomatic/20190304/bin\") module whatis R/CRAN/3.5 R/CRAN/3.5 : Adds the R library path to the pre-built CRAN modules","title":"Preparing your environment"},{"location":"environment/#adding-or-loading-software","text":"Once you have found the module path you can load the software: module load lua/5.3.5 After the module loads you can type srun --pty lua at a prompt, or add it to the path of your lua script (the RC team recommends using /usr/bin/env instead of an absolute path). Showing/listing the module environment modifications You can discover what the module will load into your environment you can run module show, for example here is what R adds: module show R/3.5.1 -------------------------------------------------- /home/software/tools/modulefiles/R/3.5.1: -------------------------------------------------- whatis(\"Adds the R language path to your environment \") prepend_path(\"PATH\",\"/home/software/apps/R/3.5.1/bin\")","title":"Adding or loading software"},{"location":"environment/#listing-loaded-modules","text":"To see what modules you have loaded into your environment you can run the command: module list By default you will have the config module loaded (please do not unload that module). For example, here are the modules I have loaded in my environment when I wrote this section: module list Currently Loaded Modules: 1) config 2) tassel/3 3) python/3.7.0 4) python/modules/3.7","title":"Listing loaded modules"},{"location":"examples/","text":"Examples \u00b6 Simple Bash Example - start here if new to HPC \u00b6 In this example we will run a very simple bash script on the quicktest partition. The bash script is very simple, it just prints the hostname - the node you're running on - and prints the date into a file. It also sleeps for 1 minute - it just does this to give you a chance to see your job in the queue with squeue First lets create a sensible working directory mkdir bash_example cd bash_example We'll use the text editor nano to create our bash script as well as our submission script. In real life, you might find it easier to create your code and submission script on your local machine, then copy them over as nano is not a great editor for large projects. Create and edit our simple bash script - this is our code we will run on the HPC nano test.sh Paste or type the following into the file #!/bin/bash hostname #prints the host name to the terminal date > date_when_job_ran.txt #puts the content of the date command into a txt file sleep 1m # do nothing for 1 minute. Job will still be \"running\" press ctrl-O to save the text in nano, then ctrl-X to exit nano. Using nano again create a file called submit.sh with the following content #!/bin/bash # #SBATCH --job-name=bash_test #SBATCH -o bash_test.out #SBATCH -e bash_test.err # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --time=10:00 bash test.sh #actually run our bash script, using bash If you're familiar with bash scripts, the above is a bit weird. The #SBATCH lines would normally be comments and hence not do anything, but Slurm will read those lines to determine how many resources to provide your job. In this case we ask for the following: quicktest partition (the default - so you don't technically need to ask for it). 1 cpu per task - we have one task, so we're asking for 1 cpu 1 gig of memory. a max runtime of 10 min If your job uses more memory or time than requested, Slurm will immediately kill it. If you use more CPU's than requested - your job will keep running, but your \"cpus\" will be shared bewteen the CPUs you actually requested. So if your job tried to use 10 CPUs but you only asked for one, it'll run extremely slowly - don't do this. Our submit.sh script also names our job bash_test this is what the job will show up as in squeue. We ask for things printed out on the terminal to go to two seperate files. Normal, non error, things that would be printed out on the terminal will be put into the text file bash_test.out . Errors will be printed into the text file bash_test.err Now submit your job to the Slurm queue. sbatch submit.sh #See your job in the queue squeue -u <your_username> #When job is done see the new files ls #look at the content that would have been printed to the terminal if running locally cat bash_test.out # See the content of the file that your bash script created cat date_when_job_ran.txt Simple Python program using virtualenv and pip \u00b6 First we need to create a working directory and move there mkdir python_test cd python_test Next we load the python 3 module and use python 3 to create a python virtualenv. This way we can install pip packages which are not installed on the cluster module load python/3.6.6 python3 -m venv mytest Activate the mytest virtualenv and use pip to install the webcolors package source mytest/bin/activate pip install webcolors Create the file test.py with the following contents using nano import webcolors from random import randint from socket import gethostname colour_list = list ( webcolors . CSS3_HEX_TO_NAMES . items ()) requested_colour = randint ( 0 , len ( colour_list )) colour_name = colour_list [ requested_colour ][ 1 ] print ( \"Random colour name:\" , colour_name , \" on host: \" , gethostname ()) Alternatively download it with wget: wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/test.py Using nano create the submissions script called python_submit.sh with the following content - change me@email.com to your email address. #!/bin/bash # #SBATCH --job-name=python_test #SBATCH -o python_test.out #SBATCH -e python_test.err # #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1G #SBATCH --time=10:00 # #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load python/3.6.6 source mytest/bin/activate python test.py Alternatively download it with wget wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/python_submit.sh To submit your job to the Slurm scheduler sbatch python_submit.sh Check for your job on the queue with squeue though it might finish very fast. The output files will appear in your working directory. Using Anaconda/Miniconda/conda - idba \u00b6 Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is available if you want a more minimal initial setup. module load old-mod-system/Anaconda3/2020.11 Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct conda activate idba-example #activate our example environment. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/andre/anaconda3 idba-example /home/andre/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load old-mod-system/Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/andre/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out Loading R packages & running a simple job \u00b6 First login to R\u0101poi and load the R and R/CRAN modules: module load R/4.0.2 module load R/CRAN Then run R on the command line: R Test library existence: > library ( tidyverse ) This should load the package, and give some output like this: \u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.3 . 0 \u2500\u2500 \u2714 ggplot2 3.3 . 2 \u2714 purrr 0.3 . 4 \u2714 tibble 3.0 . 1 \u2714 dplyr 1.0 . 0 \u2714 tidyr 1.1 . 0 \u2714 stringr 1.4 . 0 \u2714 readr 1.3 . 1 \u2714 forcats 0.5 . 0 \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts () \u2500\u2500 \u2716 dplyr :: filter () masks stats :: filter () \u2716 dplyr :: lag () masks stats :: lag () (These conflicts are normal and can be ignored.) To quit R, type: > q () Next create a bash submission script called r_submit.sh (or another name of your choice) using your preferred text editor, e.g. nano. #!/bin/bash # #SBATCH --job-name=r_test #SBATCH -o r_test.out #SBATCH -e r_test.err # #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1G #SBATCH --time=10:00 # module load R/4.0.2 module load R/CRAN Rscript mytest.R Save this to the current working directory, and then create another file using your preferred text editor called mytest.R (or another name of your choice) containing the following R commands: library ( tidyverse ) sprintf ( \"Hello World!\" ) then run it with the previously written bash script: sbatch r_submit.sh This submits a task that should execute quickly and create files in the directory from which it was run. Examine r_test.out . You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. You should see: \"Hello World\" Matlab GPU example \u00b6 Matlab has various built-in routines which are GPU accelerated. We will run a simple speed comparison between cpu and gpu tasks. In a sensible location create a file called matlab_gpu.m I used ~/examples/matlab/cuda/matlab_gpu.m . % Set an array which will calculate the Eigenvalues of A = rand ( 1000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc We will also need a Slurm submission script; we'll call this matlab_gpu.sh . Note that we will need to use the new Easybuild module files for our cuda libraries, so make sure to include the module use line module use /home/software/tools/eb_modulefiles/all/Core #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module use /home/software/tools/eb_modulefiles/all/Core module load matlab/2021a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" To submit this job to the Slurm queue sbatch matlab_gpu.sh . This job will take a few minutes to run - this is mostly the Matlab startup time. Examine the queue for your job squeue -u $USER . When your job is done, inspect the output file. You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. cat out-gpu-example.out What do you notice about the output? Surely GPUs should be faster than the CPU! It takes time for the GPU to start processing your task, the CPU is able to start the task far more quickly. So for short operations, the CPU can be faster than the GPU - remember to benchmark your code for optimal performance! Just because you can use a GPU for your task doesn't mean it is necessarily faster! To get a better idea of the advantage of the GPU let's increase the size of the array from 1000 to 10000 matlab_gpu.m % Set an array which will calculate the Eigenvalues of A = rand ( 10000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc To make things fairer for the CPU in this case, we will also allocate half the CPUs on the node to Matlab. Half the CPUs, half the memory and half the GPUs, just to be fair. matlab_gpu.sh #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=128 #SBATCH --mem=256G module use /home/software/tools/eb_modulefiles/all/Core module load matlab/2021a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" The output in my case was: < M A T L A B ( R ) > Copyright 1984 -2021 The MathWorks, Inc. R2021a Update 1 ( 9 .10.0.1649659 ) 64 -bit ( glnxa64 ) April 13 , 2021 To get started, type doc. For product information, visit www.mathworks.com. t1 = 62 .0212 t2 = 223 .0818 So in thise case the GPU was considerably faster. Matlab can do this a bit faster on the CPU if you give it fewer CPUs, the optimum appears to be around 20, but it still takes 177s. Again, optimise your resource requests for your problem, less can sometimes be more, however the GPU easily wins in this case. Job Arrays - running many similar jobs \u00b6 Slurm makes it easy to run many jobs which are similar to each other. This could be one piece of code running over many datasets in parallel or running a set of simulations with a different set of parameters for each run. Simple Bash Job Array example \u00b6 The following code will run the submission script 16 times as resources become available (i.e. they will not neccesarily run at the same time). It will just print out the Slurm array task ID and exit. submit.sh: #!/bin/bash #SBATCH --job-name=test_array #SBATCH --output=out_array_%A_%a.out #SBATCH --error=out_array_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G # Print the task id. echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Add lines here to run your computations. Run the example with the standard sbatch submit.sh A simple R job Array Example \u00b6 As a slightly more practical example the following will run an R script 5 times as resources become available. The R script takes as an input the $SLURM_ARRAY_TASK_ID which then selects a parameter alpha out of a lookup table. This is one way you could run simulations or similar with a set parameters defined in a lookuop table in your code. To make outputs more tidy and to help organisation, instead of dumping all the outputs into the directory with our code and submission script, we will separate the outputs into directories. Dataframes saved from R will be saved to the output/ directory, and all output which would otherwise be printed to the commnd line (stdout and stderr) will be saved to the stdout/ directory. Both of these directories will need to be created before running the script. r_random_alpha.R: # get the arguments supplied to R. # trailingOnly = TRUE gets the user supplied # arguments, and for now we will only get the # first user supplied argument args <- commandArgs ( trailingOnly = TRUE ) inputparam <- args [ 1 ] # a vector with all our parameters. alpha_vec <- c ( 2.5 , 3.3 , 5.1 , 8.2 , 10.9 ) alpha <- alpha_vec [ as.integer ( inputparam )] # Generate a random number between 0 and alpha # store it in dataframe with the coresponding # alpha value randomnum <- runif ( 1 , min = 0 , max = as.double ( alpha )) df <- data.frame ( \"alpha\" = alpha , \"random_num\" = randomnum ) # Save the data frame to a file with the alpha value # Note that the output/ folder will need to be # manually created first! outputname <- paste ( \"output/\" , \"alpha_\" , alpha , \".Rda\" , sep = \"\" ) save ( df , file = outputname ) Next create the submision script. Which we will run on the parallel partition rather than quicktest. r_submit.sh: #!/bin/bash #SBATCH --job-name=test_R_array #SBATCH --output=stdout/array_%A_%a.out #SBATCH --error=stdout/array_%A_%a.err #SBATCH --array=1-5 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G module load R/CRAN # Print the task id. Rscript r_random_alpha.R $SLURM_ARRAY_TASK_ID Run the jobs with sbatch r_submit.sh Singularity \u00b6 While there are many modules on R\u0101poi, sometimes you might want to install your own packages in your own way. Singularity allows you to do this. If you are familiar with Docker, Singularity is similar, except you can't get root (or sudo) once your container is running on the R\u0101poi. However, you can have sudo rights locally on your own machine, setup your container however you like, then run it without sudo on the cluster. Singularity/Docker container example \u00b6 Singularity allows you to use most (but not all!) docker images on R\u0101poi. On your local machine create the singularity definition file input_args_example.def BootStrap : library From : ubuntu: 16.04 %runscript exec echo \" $@ \" %labels Author Andre This will build an ubuntu 16.04 container that will eventually run on R\u0101poi which runs Centos. This container has a runscript which just echos back any arguments sent to the container when your start it up. Build the container locally with sudo and singularity sudo singularity build inputexample.sif input_args_example.def This will build an image that you can't modify any further and is immediately suitable to run on R\u0101poi Copy this file to R\u0101poi via sftp sftp <username>@raapoi.vuw.ac.nz Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:00:20 #SBATCH --ntasks=1 #SBATCH --mem=1G module load singularity singularity run inputtest.sif \"hello from a container\" Run the script with the usual singularity_submit.sh Singularity/TensorFlow Example \u00b6 tensor.def Bootstrap: docker From: tensorflow/tensorflow:latest-py3 %post apt-get update && apt-get -y install wget build-essential %runscript exec python \" $@ \" compile this locally with sudo and singularity. sudo singularity build tensorflow.sif tensor.def Create a quick tensorflow test code tensortest.py import tensorflow as tf mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 5 ) model . evaluate ( x_test , y_test ) Copy your files to R\u0101poi via sftp (or whatever you prefer) sftp <username>@raapoi.vuw.ac.nz cd <where you want to work> put * #put all files in your local directory onto R\u0101poi Lets quickly test the code via an interactive session on a node. Note I find the tensorflow container only runs properly on intel nodes, which we don't have many of at the moment, I'll investigate this further. srun --partition = \"parallel\" --constraint = \"Intel\" --pty bash #now on the remote node - note you might need to wait if nodes are busy module load singularity #load singularity singularity shell tensorflow.sif #now inside the tensorflow container on the remote node python tensortest.py #once that runs, exit the container exit #exit the container exit #exit the interactive session on the node Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --constraint=Intel #SBATCH --ntasks=1 #SBATCH --mem=4G module load singularity #run the container with the runscript defined when we created it singularity run tensorflow.sif tensortest.py Singularity/MaxBin2 Example \u00b6 In a sensible location, either in your home directory or on the scratch: Get the maxbin2 container, there are a few places to get this, but will get the bioconda container as it is more recent than the one referenced on the official maxbin site. module load module load singularity singularity pull docker://quay.io/biocontainers/maxbin2:2.2.6--h14c3975_0 mv maxbin2_2.2.6--h14c3975_0.sif maxbin2_2.2.6.sif #rename for convenience Download some test data mkdir rawdata curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.scaffold > rawdata/20x.scaffold curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.abund > rawdata/20x.abund Create an output data location mkdir output Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=maxbin2_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --ntasks=4 #SBATCH --mem=4G module load singularity singularity exec maxbin2_2.2.6.sif run_MaxBin.pl -contig rawdata/20x.scaffold -abund rawdata/20x.abund -out output/20x.out -thread 4 Singularity/Sandbox Example \u00b6 This lets you have root inside a container locally and make changes to it. This is really handy for determining how to setuop your container. While you can convert the sandbox container to one you can run on R\u0101poi, I suggest you don't do this . Use the sandbox to figure out how you need to configure your container, what packages to install, config files to change etc. Then create a .def file that contains all the nessesary steps without the need to use the sandbox - this will make your work more reproducable and easier to share with others. example.def BootStrap: library From: ubuntu:16.04 %post apt-get update && apt-get -y install wget build-essential %runscript exec echo \" $@ \" %labels Author Andre Compile this locally with sudo and singularity. We are using the sandbox flag to create a writable container directory ( example/ ) on our local machine where we have sudo rights. sudo singularity build --sandbox example/ example.def Now we can run the container we just built, but with sudo rights inside the container. Your rights outside the container match the rights inside the container, so we need to do this with sudo. sudo singularity shell --writable example/ Inside the container we now have root and can install packages and modify files in the root directories Singularity example:~> apt update Singularity example:~> apt install sqlite Singularity example:~> touch /test.txt #create an empty file in root Singularity example:~> ls / Singularity example:~> exit #exit container To run the container on R\u0101poi we convert it to the default immutable image with build. We might need sudo for this as the prior use of sudo will have created a directory that your usual user can't see every file. sudo singularity build new-example-sif example/ You could now copy the new-example-sif file to R\u0101poi and run it there. However a better workflow is to use this to experiment, to find out what changes you need to make to the image and what packages you need to install. Once you've done that, I suggest starting afresh and putting everything in the.def file . That way when you return to your project in 6 months, or hand it over to someone else, there is a clear record of how the image was built. Singularity/Custom Conda Container - idba example \u00b6 In this example we'll build a singularity container using conda. The example is building a container for idba - a genome assembler. Idba is available in bioconda, but not as a biocontainer. We'll build this container locally to match a local conda environment, then run it on the HPC and do an example assembly. Locally \u00b6 Make sure you have conda setup on your local machine, anaconda and miniconda are good choices. Create a new conda environment and install idba conda create --name idba conda install -c bioconda idba Export your conda environment, we will use this to build the container. conda env export > environment.yml We will use a singularity definition, basing our build on a docker miniconda image. There is a bunch of stuff in this file to make sure the conda environment is in the path. From stackoverflow idba.def Bootstrap: docker From: continuumio/miniconda3 %files environment.yml %environment PATH=/opt/conda/envs/$(head -1 environment.yml | cut -d' ' -f2)/bin:$PATH %post echo \". /opt/conda/etc/profile.d/conda.sh\" >> ~/.bashrc echo \"source activate $(head -1 environment.yml | cut -d' ' -f2)\" > ~/.bashrc /opt/conda/bin/conda env create -f environment.yml %runscript exec \"$@\" Build the image sudo singularity build idba.img idba.def Now copy the idba.img and environment.yml (technically the environment file is not needed, but not having it creates a warning) to somewhere sensible on R\u0101poi. On R\u0101poi \u00b6 Create a data directory, so we can separate our inputs and outputs. Download a paired end illumina read of Ecoli from S3 with wget. The data comes from the Illumina public data library mkdir data cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired end fastq files but idba requires a fasta file. We can use a tool built into our container to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. module load singularity singularity exec fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 1G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o output.out #SBATCH -e output.err #SBATCH --time=00:10:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=1G module load singularity singularity exec idba.img idba idba_ud -r data/read.fa -o output Now we can submit our script to the queue with sbatch idba_submit.sh","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#simple-bash-example-start-here-if-new-to-hpc","text":"In this example we will run a very simple bash script on the quicktest partition. The bash script is very simple, it just prints the hostname - the node you're running on - and prints the date into a file. It also sleeps for 1 minute - it just does this to give you a chance to see your job in the queue with squeue First lets create a sensible working directory mkdir bash_example cd bash_example We'll use the text editor nano to create our bash script as well as our submission script. In real life, you might find it easier to create your code and submission script on your local machine, then copy them over as nano is not a great editor for large projects. Create and edit our simple bash script - this is our code we will run on the HPC nano test.sh Paste or type the following into the file #!/bin/bash hostname #prints the host name to the terminal date > date_when_job_ran.txt #puts the content of the date command into a txt file sleep 1m # do nothing for 1 minute. Job will still be \"running\" press ctrl-O to save the text in nano, then ctrl-X to exit nano. Using nano again create a file called submit.sh with the following content #!/bin/bash # #SBATCH --job-name=bash_test #SBATCH -o bash_test.out #SBATCH -e bash_test.err # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --time=10:00 bash test.sh #actually run our bash script, using bash If you're familiar with bash scripts, the above is a bit weird. The #SBATCH lines would normally be comments and hence not do anything, but Slurm will read those lines to determine how many resources to provide your job. In this case we ask for the following: quicktest partition (the default - so you don't technically need to ask for it). 1 cpu per task - we have one task, so we're asking for 1 cpu 1 gig of memory. a max runtime of 10 min If your job uses more memory or time than requested, Slurm will immediately kill it. If you use more CPU's than requested - your job will keep running, but your \"cpus\" will be shared bewteen the CPUs you actually requested. So if your job tried to use 10 CPUs but you only asked for one, it'll run extremely slowly - don't do this. Our submit.sh script also names our job bash_test this is what the job will show up as in squeue. We ask for things printed out on the terminal to go to two seperate files. Normal, non error, things that would be printed out on the terminal will be put into the text file bash_test.out . Errors will be printed into the text file bash_test.err Now submit your job to the Slurm queue. sbatch submit.sh #See your job in the queue squeue -u <your_username> #When job is done see the new files ls #look at the content that would have been printed to the terminal if running locally cat bash_test.out # See the content of the file that your bash script created cat date_when_job_ran.txt","title":"Simple Bash Example - start here if new to HPC"},{"location":"examples/#simple-python-program-using-virtualenv-and-pip","text":"First we need to create a working directory and move there mkdir python_test cd python_test Next we load the python 3 module and use python 3 to create a python virtualenv. This way we can install pip packages which are not installed on the cluster module load python/3.6.6 python3 -m venv mytest Activate the mytest virtualenv and use pip to install the webcolors package source mytest/bin/activate pip install webcolors Create the file test.py with the following contents using nano import webcolors from random import randint from socket import gethostname colour_list = list ( webcolors . CSS3_HEX_TO_NAMES . items ()) requested_colour = randint ( 0 , len ( colour_list )) colour_name = colour_list [ requested_colour ][ 1 ] print ( \"Random colour name:\" , colour_name , \" on host: \" , gethostname ()) Alternatively download it with wget: wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/test.py Using nano create the submissions script called python_submit.sh with the following content - change me@email.com to your email address. #!/bin/bash # #SBATCH --job-name=python_test #SBATCH -o python_test.out #SBATCH -e python_test.err # #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1G #SBATCH --time=10:00 # #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load python/3.6.6 source mytest/bin/activate python test.py Alternatively download it with wget wget https://raw.githubusercontent.com/ \\ vuw-research-computing/raapoi-tools/ \\ master/examples/python_venv/python_submit.sh To submit your job to the Slurm scheduler sbatch python_submit.sh Check for your job on the queue with squeue though it might finish very fast. The output files will appear in your working directory.","title":"Simple Python program using virtualenv and pip"},{"location":"examples/#using-anacondaminicondaconda-idba","text":"Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is available if you want a more minimal initial setup. module load old-mod-system/Anaconda3/2020.11 Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct conda activate idba-example #activate our example environment. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/andre/anaconda3 idba-example /home/andre/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load old-mod-system/Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/andre/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Using Anaconda/Miniconda/conda - idba"},{"location":"examples/#loading-r-packages-running-a-simple-job","text":"First login to R\u0101poi and load the R and R/CRAN modules: module load R/4.0.2 module load R/CRAN Then run R on the command line: R Test library existence: > library ( tidyverse ) This should load the package, and give some output like this: \u2500\u2500 Attaching packages \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse 1.3 . 0 \u2500\u2500 \u2714 ggplot2 3.3 . 2 \u2714 purrr 0.3 . 4 \u2714 tibble 3.0 . 1 \u2714 dplyr 1.0 . 0 \u2714 tidyr 1.1 . 0 \u2714 stringr 1.4 . 0 \u2714 readr 1.3 . 1 \u2714 forcats 0.5 . 0 \u2500\u2500 Conflicts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 tidyverse_conflicts () \u2500\u2500 \u2716 dplyr :: filter () masks stats :: filter () \u2716 dplyr :: lag () masks stats :: lag () (These conflicts are normal and can be ignored.) To quit R, type: > q () Next create a bash submission script called r_submit.sh (or another name of your choice) using your preferred text editor, e.g. nano. #!/bin/bash # #SBATCH --job-name=r_test #SBATCH -o r_test.out #SBATCH -e r_test.err # #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=1G #SBATCH --time=10:00 # module load R/4.0.2 module load R/CRAN Rscript mytest.R Save this to the current working directory, and then create another file using your preferred text editor called mytest.R (or another name of your choice) containing the following R commands: library ( tidyverse ) sprintf ( \"Hello World!\" ) then run it with the previously written bash script: sbatch r_submit.sh This submits a task that should execute quickly and create files in the directory from which it was run. Examine r_test.out . You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. You should see: \"Hello World\"","title":"Loading R packages &amp; running a simple job"},{"location":"examples/#matlab-gpu-example","text":"Matlab has various built-in routines which are GPU accelerated. We will run a simple speed comparison between cpu and gpu tasks. In a sensible location create a file called matlab_gpu.m I used ~/examples/matlab/cuda/matlab_gpu.m . % Set an array which will calculate the Eigenvalues of A = rand ( 1000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc We will also need a Slurm submission script; we'll call this matlab_gpu.sh . Note that we will need to use the new Easybuild module files for our cuda libraries, so make sure to include the module use line module use /home/software/tools/eb_modulefiles/all/Core #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module use /home/software/tools/eb_modulefiles/all/Core module load matlab/2021a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" To submit this job to the Slurm queue sbatch matlab_gpu.sh . This job will take a few minutes to run - this is mostly the Matlab startup time. Examine the queue for your job squeue -u $USER . When your job is done, inspect the output file. You can use an editor like nano, vi or emacs, or you can just cat or less the file to see its contents on the terminal. cat out-gpu-example.out What do you notice about the output? Surely GPUs should be faster than the CPU! It takes time for the GPU to start processing your task, the CPU is able to start the task far more quickly. So for short operations, the CPU can be faster than the GPU - remember to benchmark your code for optimal performance! Just because you can use a GPU for your task doesn't mean it is necessarily faster! To get a better idea of the advantage of the GPU let's increase the size of the array from 1000 to 10000 matlab_gpu.m % Set an array which will calculate the Eigenvalues of A = rand ( 10000 ); % Copy the Array to the GPU memory - this process takes an erratic amount of time, so we will not time it. Agpu = gpuArray ( A ); tic B=eig(Agpu) ; t1 = toc % Let's compare the time with CPU tic B = eig ( A ); t2 = toc To make things fairer for the CPU in this case, we will also allocate half the CPUs on the node to Matlab. Half the CPUs, half the memory and half the GPUs, just to be fair. matlab_gpu.sh #!/bin/bash #SBATCH --job-name=matlab-gpu-example #SBATCH --output=out-gpu-example.out #SBATCH --error=out-gpu-example.err #SBATCH --time=00:05:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=128 #SBATCH --mem=256G module use /home/software/tools/eb_modulefiles/all/Core module load matlab/2021a module load fosscuda/2020b matlab -nodisplay -nosplash -nodesktop -r \"run('matlab_gpu.m');exit;\" The output in my case was: < M A T L A B ( R ) > Copyright 1984 -2021 The MathWorks, Inc. R2021a Update 1 ( 9 .10.0.1649659 ) 64 -bit ( glnxa64 ) April 13 , 2021 To get started, type doc. For product information, visit www.mathworks.com. t1 = 62 .0212 t2 = 223 .0818 So in thise case the GPU was considerably faster. Matlab can do this a bit faster on the CPU if you give it fewer CPUs, the optimum appears to be around 20, but it still takes 177s. Again, optimise your resource requests for your problem, less can sometimes be more, however the GPU easily wins in this case.","title":"Matlab GPU example"},{"location":"examples/#job-arrays-running-many-similar-jobs","text":"Slurm makes it easy to run many jobs which are similar to each other. This could be one piece of code running over many datasets in parallel or running a set of simulations with a different set of parameters for each run.","title":"Job Arrays - running many similar jobs"},{"location":"examples/#simple-bash-job-array-example","text":"The following code will run the submission script 16 times as resources become available (i.e. they will not neccesarily run at the same time). It will just print out the Slurm array task ID and exit. submit.sh: #!/bin/bash #SBATCH --job-name=test_array #SBATCH --output=out_array_%A_%a.out #SBATCH --error=out_array_%A_%a.err #SBATCH --array=1-16 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G # Print the task id. echo \"My SLURM_ARRAY_TASK_ID: \" $SLURM_ARRAY_TASK_ID # Add lines here to run your computations. Run the example with the standard sbatch submit.sh","title":"Simple Bash Job Array example"},{"location":"examples/#a-simple-r-job-array-example","text":"As a slightly more practical example the following will run an R script 5 times as resources become available. The R script takes as an input the $SLURM_ARRAY_TASK_ID which then selects a parameter alpha out of a lookup table. This is one way you could run simulations or similar with a set parameters defined in a lookuop table in your code. To make outputs more tidy and to help organisation, instead of dumping all the outputs into the directory with our code and submission script, we will separate the outputs into directories. Dataframes saved from R will be saved to the output/ directory, and all output which would otherwise be printed to the commnd line (stdout and stderr) will be saved to the stdout/ directory. Both of these directories will need to be created before running the script. r_random_alpha.R: # get the arguments supplied to R. # trailingOnly = TRUE gets the user supplied # arguments, and for now we will only get the # first user supplied argument args <- commandArgs ( trailingOnly = TRUE ) inputparam <- args [ 1 ] # a vector with all our parameters. alpha_vec <- c ( 2.5 , 3.3 , 5.1 , 8.2 , 10.9 ) alpha <- alpha_vec [ as.integer ( inputparam )] # Generate a random number between 0 and alpha # store it in dataframe with the coresponding # alpha value randomnum <- runif ( 1 , min = 0 , max = as.double ( alpha )) df <- data.frame ( \"alpha\" = alpha , \"random_num\" = randomnum ) # Save the data frame to a file with the alpha value # Note that the output/ folder will need to be # manually created first! outputname <- paste ( \"output/\" , \"alpha_\" , alpha , \".Rda\" , sep = \"\" ) save ( df , file = outputname ) Next create the submision script. Which we will run on the parallel partition rather than quicktest. r_submit.sh: #!/bin/bash #SBATCH --job-name=test_R_array #SBATCH --output=stdout/array_%A_%a.out #SBATCH --error=stdout/array_%A_%a.err #SBATCH --array=1-5 #SBATCH --time=00:00:20 #SBATCH --partition=parallel #SBATCH --ntasks=1 #SBATCH --mem=1G module load R/CRAN # Print the task id. Rscript r_random_alpha.R $SLURM_ARRAY_TASK_ID Run the jobs with sbatch r_submit.sh","title":"A simple R job Array Example"},{"location":"examples/#singularity","text":"While there are many modules on R\u0101poi, sometimes you might want to install your own packages in your own way. Singularity allows you to do this. If you are familiar with Docker, Singularity is similar, except you can't get root (or sudo) once your container is running on the R\u0101poi. However, you can have sudo rights locally on your own machine, setup your container however you like, then run it without sudo on the cluster.","title":"Singularity"},{"location":"examples/#singularitydocker-container-example","text":"Singularity allows you to use most (but not all!) docker images on R\u0101poi. On your local machine create the singularity definition file input_args_example.def BootStrap : library From : ubuntu: 16.04 %runscript exec echo \" $@ \" %labels Author Andre This will build an ubuntu 16.04 container that will eventually run on R\u0101poi which runs Centos. This container has a runscript which just echos back any arguments sent to the container when your start it up. Build the container locally with sudo and singularity sudo singularity build inputexample.sif input_args_example.def This will build an image that you can't modify any further and is immediately suitable to run on R\u0101poi Copy this file to R\u0101poi via sftp sftp <username>@raapoi.vuw.ac.nz Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:00:20 #SBATCH --ntasks=1 #SBATCH --mem=1G module load singularity singularity run inputtest.sif \"hello from a container\" Run the script with the usual singularity_submit.sh","title":"Singularity/Docker container example"},{"location":"examples/#singularitytensorflow-example","text":"tensor.def Bootstrap: docker From: tensorflow/tensorflow:latest-py3 %post apt-get update && apt-get -y install wget build-essential %runscript exec python \" $@ \" compile this locally with sudo and singularity. sudo singularity build tensorflow.sif tensor.def Create a quick tensorflow test code tensortest.py import tensorflow as tf mnist = tf . keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 512 , activation = tf . nn . relu ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 , activation = tf . nn . softmax ) ]) model . compile ( optimizer = 'adam' , loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( x_train , y_train , epochs = 5 ) model . evaluate ( x_test , y_test ) Copy your files to R\u0101poi via sftp (or whatever you prefer) sftp <username>@raapoi.vuw.ac.nz cd <where you want to work> put * #put all files in your local directory onto R\u0101poi Lets quickly test the code via an interactive session on a node. Note I find the tensorflow container only runs properly on intel nodes, which we don't have many of at the moment, I'll investigate this further. srun --partition = \"parallel\" --constraint = \"Intel\" --pty bash #now on the remote node - note you might need to wait if nodes are busy module load singularity #load singularity singularity shell tensorflow.sif #now inside the tensorflow container on the remote node python tensortest.py #once that runs, exit the container exit #exit the container exit #exit the interactive session on the node Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=singularity_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --constraint=Intel #SBATCH --ntasks=1 #SBATCH --mem=4G module load singularity #run the container with the runscript defined when we created it singularity run tensorflow.sif tensortest.py","title":"Singularity/TensorFlow Example"},{"location":"examples/#singularitymaxbin2-example","text":"In a sensible location, either in your home directory or on the scratch: Get the maxbin2 container, there are a few places to get this, but will get the bioconda container as it is more recent than the one referenced on the official maxbin site. module load module load singularity singularity pull docker://quay.io/biocontainers/maxbin2:2.2.6--h14c3975_0 mv maxbin2_2.2.6--h14c3975_0.sif maxbin2_2.2.6.sif #rename for convenience Download some test data mkdir rawdata curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.scaffold > rawdata/20x.scaffold curl https://downloads.jbei.org/data/microbial_communities/MaxBin/getfile.php?20x.abund > rawdata/20x.abund Create an output data location mkdir output Create a submit script using singularity on the cluster singularity_submit.sh #!/bin/bash #SBATCH --job-name=maxbin2_test #SBATCH -o sing_test.out #SBATCH -e sing_test.err #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --ntasks=4 #SBATCH --mem=4G module load singularity singularity exec maxbin2_2.2.6.sif run_MaxBin.pl -contig rawdata/20x.scaffold -abund rawdata/20x.abund -out output/20x.out -thread 4","title":"Singularity/MaxBin2 Example"},{"location":"examples/#singularitysandbox-example","text":"This lets you have root inside a container locally and make changes to it. This is really handy for determining how to setuop your container. While you can convert the sandbox container to one you can run on R\u0101poi, I suggest you don't do this . Use the sandbox to figure out how you need to configure your container, what packages to install, config files to change etc. Then create a .def file that contains all the nessesary steps without the need to use the sandbox - this will make your work more reproducable and easier to share with others. example.def BootStrap: library From: ubuntu:16.04 %post apt-get update && apt-get -y install wget build-essential %runscript exec echo \" $@ \" %labels Author Andre Compile this locally with sudo and singularity. We are using the sandbox flag to create a writable container directory ( example/ ) on our local machine where we have sudo rights. sudo singularity build --sandbox example/ example.def Now we can run the container we just built, but with sudo rights inside the container. Your rights outside the container match the rights inside the container, so we need to do this with sudo. sudo singularity shell --writable example/ Inside the container we now have root and can install packages and modify files in the root directories Singularity example:~> apt update Singularity example:~> apt install sqlite Singularity example:~> touch /test.txt #create an empty file in root Singularity example:~> ls / Singularity example:~> exit #exit container To run the container on R\u0101poi we convert it to the default immutable image with build. We might need sudo for this as the prior use of sudo will have created a directory that your usual user can't see every file. sudo singularity build new-example-sif example/ You could now copy the new-example-sif file to R\u0101poi and run it there. However a better workflow is to use this to experiment, to find out what changes you need to make to the image and what packages you need to install. Once you've done that, I suggest starting afresh and putting everything in the.def file . That way when you return to your project in 6 months, or hand it over to someone else, there is a clear record of how the image was built.","title":"Singularity/Sandbox Example"},{"location":"examples/#singularitycustom-conda-container-idba-example","text":"In this example we'll build a singularity container using conda. The example is building a container for idba - a genome assembler. Idba is available in bioconda, but not as a biocontainer. We'll build this container locally to match a local conda environment, then run it on the HPC and do an example assembly.","title":"Singularity/Custom Conda Container - idba example"},{"location":"faq/","text":"Frequently Asked Questions \u00b6 I don't want to interfere with other people work, what does it mean \"Other users are prevented from using resources you request, even if you don't use them\"? The system is shared, you will very rarely have full use of a node. You need to be careful to request resources, leaving the extra space available to others. For example, say you submitted a job to bigmem which asked for 800GB of ram and 10 CPUs. Your job would end up on the node with 1000GB ram as only that one would fit it - if your job actually only used 300GB of ram - the extra 500GB of ram you requested would be \"wasted\" no one else could use it. So, another user with a job requesting 600GB of ram would have to wait for your job to end even if there was space for it to run alongside yours. The same issue occurs with CPU requests. It can be very hard to accurately estimate memory and cpu needs before running your job. If your job has a short run time (less than ~10 hours), you can just request more than you need and check the memory usage afterward to guide further jobs. If your job has a long run time (several days), you should run a test job with a short runtime (a few hours) to estimate your needs first.","title":"FAQ"},{"location":"faq/#frequently-asked-questions","text":"I don't want to interfere with other people work, what does it mean \"Other users are prevented from using resources you request, even if you don't use them\"? The system is shared, you will very rarely have full use of a node. You need to be careful to request resources, leaving the extra space available to others. For example, say you submitted a job to bigmem which asked for 800GB of ram and 10 CPUs. Your job would end up on the node with 1000GB ram as only that one would fit it - if your job actually only used 300GB of ram - the extra 500GB of ram you requested would be \"wasted\" no one else could use it. So, another user with a job requesting 600GB of ram would have to wait for your job to end even if there was space for it to run alongside yours. The same issue occurs with CPU requests. It can be very hard to accurately estimate memory and cpu needs before running your job. If your job has a short run time (less than ~10 hours), you can just request more than you need and check the memory usage afterward to guide further jobs. If your job has a long run time (several days), you should run a test job with a short runtime (a few hours) to estimate your needs first.","title":"Frequently Asked Questions"},{"location":"ganglia/","text":"Visit here to get real-time metrics and history of R\u0101poi's utilisation. Ganglia example:","title":"Ganglia"},{"location":"hpclayout/","text":"HPC layout \u00b6 Note! Understanding the R\u0101poi hardware layout is not critical for most users! It is useful for users running big parallel MPI jobs and may be of interest to others. To a first approximation a High Performance Computer (HPC) is a collection of large computers or servers (nodes) that are connected together. There will also be some attached storage. Rather than logging into the system and immediately running your program or code, it is organised into a job and submitted to a scheduler that takes your job and runs it on one of the nodes that has enough free resources (cpu and memory) to meet your job request. Most of the time you will be sharing a node with other users. It is important to try and not over request resources as requested resources are kept in reserve for you and not available to others, even if you don't use them. This is particularly important when requesting a lot of resources or running array jobs which can use up a lot of the HPCs resources. Hardware \u00b6 On R\u0101poi, the node you login into and submit your jobs to is called raapoi-master . The computers/servers making up the nodes are of several types, covered in partitions . Most of the processors in R\u0101poi are in the parallel AMD nodes such as AMD01n01, AMD01n02 etc. Figures 1-4 show more details of these nodes. Figure 1: Example of some of the computers making up R\u0101poi. This is the front panel in Rack 4 in the Datacentre - highlighted is one of the AMD chassis, which have 4 nodes each. Figure 2: Example of some of the computers making up R\u0101poi. This is the back in Rack 4 in the Datacentre. Here you can clearly see the 4 nodes in each chassis of the parallel partition Figure 3: An AMD compute node, one of 4 in a chassis. The 2 black rectangles are the processor heatsinks, on each side are the ram modules. Each ram module is 32GB for a total of 512GB. On the lower left, the green circuit board is the the InfiniBand network card. Opposite that, in black, is the 1.7TB NvMe storage we use as fast /tmp space. Figure 4: One of the CPUs with the heatsink removed. At 115.00 x 165.00 mm, it is physically much larger than the processor in a desktop Each AMD node has 2 of these 7702 processors. Each processor has 64Cores/128Threads (with SMT - symmetric multi-threading - enabled) for a total of 128Cores/256Threads per node. Network \u00b6 On R\u0101poi the nodes are connected to each other in two ways - via 10G ethernet and via 40G infiniband. Most of the time you can ignore this, but it is important for interconnected jobs running across multiple nodes like weather simulations. In figure 5 we can see the network layout of R\u0101poi from the perspective of the Login node. This is the node you ssh into, via the VUW intranet - either from a locally wired connection or via the VPN. The nodes are organised into groups mostly aligning with the partition the node is in. Ethernet \u00b6 The dashed lines indicate an Ethernet connection, all nodes are connected via ethernet at either 1G or 10G depending on the node. Most of the intel nodes are only connected at 1G due to their age. The newer nodes are all 10G connected. The ethernet connection can also reach out into the wider internet for downloading updates, datasets etc. Infiniband \u00b6 Many nodes are also connected by a solid line indicating an Infiniband network connection. This connection is faster than the ethernet connection but more importantly lower latency than the ethernet connection. This helps with large interconnected (eg MPI) jobs running across multiple nodes. The latency of the interprocess communication carried over the Infiniband link can have a dramatic affect on large scale calculations which for instance need to communicate grid boundary conditions across the nodes Where infiniband is available, the scratch storage and BeeGFS storage is transmitted over the link as the latency helps with IO performance. classDiagram Parallel_AMD -- Login Parallel_AMD .. Login Parallel_Intel -- Login Parallel_Intel .. Login Quicktest -- Login Quicktest .. Login Highmem_rack4 .. Login Highmem_rack4 -- Login Highmem .. Login GPU .. Login Login .. Internet Login .. Scratch Login -- Scratch Login .. BeeGFS Login -- BeeGFS class Internet { vuw intranet wider internet } class Scratch { 100 TB raapoi_fs01 } class BeeGFS { 100TB across Bee01 Bee02 Bee03 } class Login { raapoi-master } class Parallel_AMD { amd01n01 amd01n02 amd01n03 amd01n04 - amd02n01 amd02n02 amd02n03 amd02n04 - amd03n01 amd03n02 amd03n03 amd03n04 - amd04n01 amd04n02 amd04n03 amd04n04 - amd05n01 amd05n02 amd05n03 amd05n04 - amd06n01 amd06n02 amd06n03 amd06n04 } class Parallel_Intel{ itl01n01 itl01n02 itl01n03 itl01n04 - itl02n01 itl02n02 itl02n03 itl02n04 - itl03n01 itl03n02 itl03n03 itl03n04 } class Quicktest{ itl04n01 itl04n02 itl04n03 itl04n04 } class Highmem{ high01 high02 high03 high04 high05 } class Highmem_rack4{ high06 } class GPU{ gpu01 gpu02 } Figure 5: Logical HPC layout from the perspective of the login node. Solid lines indicate ethernet connections, dashed Infiniband Looking at the HPC from the perspective of the ethernet and infiniband networks. The nodes in Figure 6 and 7 are the same as before, but we're just using the group container label to simplify the diagram. classDiagram Parallel_AMD .. Ethernet Parallel_Intel .. Ethernet Quicktest .. Ethernet Highmem_rack4 .. Ethernet Highmem .. Ethernet GPU .. Ethernet Ethernet .. Internet Ethernet .. Login Ethernet .. Scratch Ethernet .. BeeGFS class Ethernet{ 1-10Gig Connects to the wider internet } class Scratch { } class BeeGFS { } class Login { } class Parallel_AMD { } class Parallel_Intel{ } class Quicktest{ } class Highmem{ } class Highmem_rack4{ } class GPU{ } Figure 6: Logical HPC layout from the perspective of the ethernet connections. Node layout is the same as in Figure 5, but only the group headings have been retained. classDiagram Parallel_AMD -- Infiniband Parallel_Intel -- Infiniband Quicktest -- Infiniband Highmem_rack4 -- Infiniband Infiniband -- Login Infiniband -- Scratch Infiniband -- BeeGFS class Infiniband{ 56Gb/s Low latency } class Scratch { } class BeeGFS { } class Login { } class Parallel_AMD { } class Parallel_Intel{ } class Quicktest{ } class Highmem{ } class Highmem_rack4{ } class GPU{ } Figure 7: Logical HPC layout from the perspective of the Infiniband connections. Note that not all nodes are connected via the infiniband link! Node layout is the same as in Figure 5, but only the group headings have been retained. The Infiniband nodes are connected to one of two SX6036 Infiniband switches. The intel and quicktest and login nodes are connected to one switch. Everything else is connected to the the other. The switches are broadly interconnected, but there is as small latency penalty for crossing the switch.","title":"HPC Hardware Layout"},{"location":"hpclayout/#hpc-layout","text":"Note! Understanding the R\u0101poi hardware layout is not critical for most users! It is useful for users running big parallel MPI jobs and may be of interest to others. To a first approximation a High Performance Computer (HPC) is a collection of large computers or servers (nodes) that are connected together. There will also be some attached storage. Rather than logging into the system and immediately running your program or code, it is organised into a job and submitted to a scheduler that takes your job and runs it on one of the nodes that has enough free resources (cpu and memory) to meet your job request. Most of the time you will be sharing a node with other users. It is important to try and not over request resources as requested resources are kept in reserve for you and not available to others, even if you don't use them. This is particularly important when requesting a lot of resources or running array jobs which can use up a lot of the HPCs resources.","title":"HPC layout"},{"location":"hpclayout/#hardware","text":"On R\u0101poi, the node you login into and submit your jobs to is called raapoi-master . The computers/servers making up the nodes are of several types, covered in partitions . Most of the processors in R\u0101poi are in the parallel AMD nodes such as AMD01n01, AMD01n02 etc. Figures 1-4 show more details of these nodes. Figure 1: Example of some of the computers making up R\u0101poi. This is the front panel in Rack 4 in the Datacentre - highlighted is one of the AMD chassis, which have 4 nodes each. Figure 2: Example of some of the computers making up R\u0101poi. This is the back in Rack 4 in the Datacentre. Here you can clearly see the 4 nodes in each chassis of the parallel partition Figure 3: An AMD compute node, one of 4 in a chassis. The 2 black rectangles are the processor heatsinks, on each side are the ram modules. Each ram module is 32GB for a total of 512GB. On the lower left, the green circuit board is the the InfiniBand network card. Opposite that, in black, is the 1.7TB NvMe storage we use as fast /tmp space. Figure 4: One of the CPUs with the heatsink removed. At 115.00 x 165.00 mm, it is physically much larger than the processor in a desktop Each AMD node has 2 of these 7702 processors. Each processor has 64Cores/128Threads (with SMT - symmetric multi-threading - enabled) for a total of 128Cores/256Threads per node.","title":"Hardware"},{"location":"hpclayout/#network","text":"On R\u0101poi the nodes are connected to each other in two ways - via 10G ethernet and via 40G infiniband. Most of the time you can ignore this, but it is important for interconnected jobs running across multiple nodes like weather simulations. In figure 5 we can see the network layout of R\u0101poi from the perspective of the Login node. This is the node you ssh into, via the VUW intranet - either from a locally wired connection or via the VPN. The nodes are organised into groups mostly aligning with the partition the node is in.","title":"Network"},{"location":"hpclayout/#ethernet","text":"The dashed lines indicate an Ethernet connection, all nodes are connected via ethernet at either 1G or 10G depending on the node. Most of the intel nodes are only connected at 1G due to their age. The newer nodes are all 10G connected. The ethernet connection can also reach out into the wider internet for downloading updates, datasets etc.","title":"Ethernet"},{"location":"hpclayout/#infiniband","text":"Many nodes are also connected by a solid line indicating an Infiniband network connection. This connection is faster than the ethernet connection but more importantly lower latency than the ethernet connection. This helps with large interconnected (eg MPI) jobs running across multiple nodes. The latency of the interprocess communication carried over the Infiniband link can have a dramatic affect on large scale calculations which for instance need to communicate grid boundary conditions across the nodes Where infiniband is available, the scratch storage and BeeGFS storage is transmitted over the link as the latency helps with IO performance. classDiagram Parallel_AMD -- Login Parallel_AMD .. Login Parallel_Intel -- Login Parallel_Intel .. Login Quicktest -- Login Quicktest .. Login Highmem_rack4 .. Login Highmem_rack4 -- Login Highmem .. Login GPU .. Login Login .. Internet Login .. Scratch Login -- Scratch Login .. BeeGFS Login -- BeeGFS class Internet { vuw intranet wider internet } class Scratch { 100 TB raapoi_fs01 } class BeeGFS { 100TB across Bee01 Bee02 Bee03 } class Login { raapoi-master } class Parallel_AMD { amd01n01 amd01n02 amd01n03 amd01n04 - amd02n01 amd02n02 amd02n03 amd02n04 - amd03n01 amd03n02 amd03n03 amd03n04 - amd04n01 amd04n02 amd04n03 amd04n04 - amd05n01 amd05n02 amd05n03 amd05n04 - amd06n01 amd06n02 amd06n03 amd06n04 } class Parallel_Intel{ itl01n01 itl01n02 itl01n03 itl01n04 - itl02n01 itl02n02 itl02n03 itl02n04 - itl03n01 itl03n02 itl03n03 itl03n04 } class Quicktest{ itl04n01 itl04n02 itl04n03 itl04n04 } class Highmem{ high01 high02 high03 high04 high05 } class Highmem_rack4{ high06 } class GPU{ gpu01 gpu02 } Figure 5: Logical HPC layout from the perspective of the login node. Solid lines indicate ethernet connections, dashed Infiniband Looking at the HPC from the perspective of the ethernet and infiniband networks. The nodes in Figure 6 and 7 are the same as before, but we're just using the group container label to simplify the diagram. classDiagram Parallel_AMD .. Ethernet Parallel_Intel .. Ethernet Quicktest .. Ethernet Highmem_rack4 .. Ethernet Highmem .. Ethernet GPU .. Ethernet Ethernet .. Internet Ethernet .. Login Ethernet .. Scratch Ethernet .. BeeGFS class Ethernet{ 1-10Gig Connects to the wider internet } class Scratch { } class BeeGFS { } class Login { } class Parallel_AMD { } class Parallel_Intel{ } class Quicktest{ } class Highmem{ } class Highmem_rack4{ } class GPU{ } Figure 6: Logical HPC layout from the perspective of the ethernet connections. Node layout is the same as in Figure 5, but only the group headings have been retained. classDiagram Parallel_AMD -- Infiniband Parallel_Intel -- Infiniband Quicktest -- Infiniband Highmem_rack4 -- Infiniband Infiniband -- Login Infiniband -- Scratch Infiniband -- BeeGFS class Infiniband{ 56Gb/s Low latency } class Scratch { } class BeeGFS { } class Login { } class Parallel_AMD { } class Parallel_Intel{ } class Quicktest{ } class Highmem{ } class Highmem_rack4{ } class GPU{ } Figure 7: Logical HPC layout from the perspective of the Infiniband connections. Note that not all nodes are connected via the infiniband link! Node layout is the same as in Figure 5, but only the group headings have been retained. The Infiniband nodes are connected to one of two SX6036 Infiniband switches. The intel and quicktest and login nodes are connected to one switch. Everything else is connected to the the other. The switches are broadly interconnected, but there is as small latency penalty for crossing the switch.","title":"Infiniband"},{"location":"managing_jobs/","text":"Managing Jobs \u00b6 Cancelling a Job \u00b6 To cancel a job, first find the jobID, you can use the vuw-myjobs (or squeue ) command to see a list of your jobs, including jobIDs. Once you have that you can use the scancel command, eg scancel 236789 To cancel all of your jobs you can use the -u flag followed by your username: scancel -u harrelwe Viewing Job information \u00b6 Job History \u00b6 If you want to get a quick view of all the jobs completed within the last 5 days you can use the vuw-job-history command, for example: $ vuw-job-history MY JOBS WITHIN LAST 5 days JobID State JobName MaxVMSize CPUTime ------------ ---------- ---------- ---------- ---------- 2645 COMPLETED bash 00:00:22 2645.extern COMPLETED extern 0.15G 00:00:22 2645.0 COMPLETED bash 0.22G 00:00:20 2734 COMPLETED bash 00:07:40 2734.extern COMPLETED extern 0.15G 00:07:40 2734.0 COMPLETED bash 0.22G 00:07:40 Job Reports \u00b6 To view a report of your past jobs you can run vuw-job-report : $ vuw-job-report 162711 JOB REPORT FOR JOB 162711 JobName Nodes ReqMem UsedMem(GB) ReqCPUs CPUTime State Completed test-schro 1 64Gn 24 00:02.513 COMPLETED 2019-05-28T16:17:10 batch 1 64Gn 0.15G 24 00:00.210 COMPLETED 2019-05-28T16:17:10 extern 1 64Gn 0.15G 24 00:00.002 COMPLETED 2019-05-28T16:17:10 NOTE: In this example you see that I requested 64 GigaBytes of memory but only used 0.15 GB. This means that 63 GB of memory went unused, which was a waste of resources. You can also get a report of your completed jobs using the sacct command. For example if I wanted to get a report on how much memory my job used I could do the following: sacct --units=G --format=\"MaxVMSize\" -j 2156 MaxVMSize will report the maximum virtual memory (RAM plus swap space) used by my job in GigBytes ( --units=G ) -j 2156 shows the information for job ID 2156 type man sacct at a prompt in engaging to see the documentation on the sacct command Viewing jobs in the Queue \u00b6 To view your running jobs you can type vuw-myjobs eg: $ vuw-myjobs JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 7921967 quicktest bash harrelwe R 0:12 1 c03n01 As you can see I have a single job running on the node c03n01 on the quicktest partition You can see all the jobs in the queues by running the vuw-alljobs command. This will produce a very long list of jobs if the cluster is busy. Job Queuing (aka Why isn't my job running?) \u00b6 When a partition is busy, jobs will be placed in a queue. You can observe this in the vuw-myjobs and vuw-alljobs commands. The STATE of your job will be PENDING, this means it is waiting for resources or your job has been re-prioritized to allow other users access to run their jobs (this is called fair-share queueing). The resource manager will list a reason the job is pending, these reasons can include: Priority - Your job priority has been reduced to allow other users access to the cluster. If no other user with normal priority is also pending then your job will start once resources are available. Possible reasons why your priority has been lowered can include: the number of jobs you have run in the past 24-48 hours; the duration of the job and the amount of resources requested. The Slurm manager uses fair-share queuing to ensure the best use of the cluster. You can google fair-share queuing if you want to know more Resources - There are insufficient resources to start your job. Some combination of CPU, Memory, Time or other specialized resource are unavailable. Once resources are freed up your job will begin to run. Time: If you request more time than the max run-time of a partition, your job will be queued indefinitely (in other words: it will never run). Your time request must be less than or equal to the Partition Max Run-Time. Also if a special reservation is placed on the cluster, for instance prior to a scheduled maintenance, this too will reduce the available time to run your job. You can see Max Run-Time for our partitions described in this document. CAD or ITS Staff will alert all users prior to any scheduled maintenance and advise them of the time restrictions. QOSGrpCPULimit - This is a Quality of Service configuration to limit the number of CPUs per user. The QOSMax is the maximum that can be requested for any single job. If a user requests more CPUs than the QOSMax for a single job then the job will not run. If the user requests more than QOSMax in 2 or more jobs then the subsequent jobs will queue until the users running jobs complete. PartitionTimeLimit - This means you have requested more time than the maximum runtime of the partition. This document contains information about the different partitions, including max run-time. Typing vuw-partition will also show the max run-time for the partitions available to you. ReqNodeNotAvail - 99% of the time you will receive this code if you have asked for too much time. This frequently occurs when the cluster is about to go into maintenance and a reservation has been placed on the cluster, which reduces the maximum run-time of all jobs. For example, if maintenance on the cluster is 1 week away, the maximum run-time on all jobs needs to be less than 1 week, regardless if the configured maximum run-time on a partition is greater than 1 week. To request time you can use the --time parameter. Another issue is if you request too much memory or a CPU configuration that does not exist on any node in a partition. Required node not available (down, drained or reserved) - This is related to ReqNodeNotAvail, see above.","title":"Managing Jobs"},{"location":"managing_jobs/#managing-jobs","text":"","title":"Managing Jobs"},{"location":"managing_jobs/#cancelling-a-job","text":"To cancel a job, first find the jobID, you can use the vuw-myjobs (or squeue ) command to see a list of your jobs, including jobIDs. Once you have that you can use the scancel command, eg scancel 236789 To cancel all of your jobs you can use the -u flag followed by your username: scancel -u harrelwe","title":"Cancelling a Job"},{"location":"managing_jobs/#viewing-job-information","text":"","title":"Viewing Job information"},{"location":"managing_jobs/#viewing-jobs-in-the-queue","text":"To view your running jobs you can type vuw-myjobs eg: $ vuw-myjobs JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 7921967 quicktest bash harrelwe R 0:12 1 c03n01 As you can see I have a single job running on the node c03n01 on the quicktest partition You can see all the jobs in the queues by running the vuw-alljobs command. This will produce a very long list of jobs if the cluster is busy.","title":"Viewing jobs in the Queue"},{"location":"managing_jobs/#job-queuing-aka-why-isnt-my-job-running","text":"When a partition is busy, jobs will be placed in a queue. You can observe this in the vuw-myjobs and vuw-alljobs commands. The STATE of your job will be PENDING, this means it is waiting for resources or your job has been re-prioritized to allow other users access to run their jobs (this is called fair-share queueing). The resource manager will list a reason the job is pending, these reasons can include: Priority - Your job priority has been reduced to allow other users access to the cluster. If no other user with normal priority is also pending then your job will start once resources are available. Possible reasons why your priority has been lowered can include: the number of jobs you have run in the past 24-48 hours; the duration of the job and the amount of resources requested. The Slurm manager uses fair-share queuing to ensure the best use of the cluster. You can google fair-share queuing if you want to know more Resources - There are insufficient resources to start your job. Some combination of CPU, Memory, Time or other specialized resource are unavailable. Once resources are freed up your job will begin to run. Time: If you request more time than the max run-time of a partition, your job will be queued indefinitely (in other words: it will never run). Your time request must be less than or equal to the Partition Max Run-Time. Also if a special reservation is placed on the cluster, for instance prior to a scheduled maintenance, this too will reduce the available time to run your job. You can see Max Run-Time for our partitions described in this document. CAD or ITS Staff will alert all users prior to any scheduled maintenance and advise them of the time restrictions. QOSGrpCPULimit - This is a Quality of Service configuration to limit the number of CPUs per user. The QOSMax is the maximum that can be requested for any single job. If a user requests more CPUs than the QOSMax for a single job then the job will not run. If the user requests more than QOSMax in 2 or more jobs then the subsequent jobs will queue until the users running jobs complete. PartitionTimeLimit - This means you have requested more time than the maximum runtime of the partition. This document contains information about the different partitions, including max run-time. Typing vuw-partition will also show the max run-time for the partitions available to you. ReqNodeNotAvail - 99% of the time you will receive this code if you have asked for too much time. This frequently occurs when the cluster is about to go into maintenance and a reservation has been placed on the cluster, which reduces the maximum run-time of all jobs. For example, if maintenance on the cluster is 1 week away, the maximum run-time on all jobs needs to be less than 1 week, regardless if the configured maximum run-time on a partition is greater than 1 week. To request time you can use the --time parameter. Another issue is if you request too much memory or a CPU configuration that does not exist on any node in a partition. Required node not available (down, drained or reserved) - This is related to ReqNodeNotAvail, see above.","title":"Job Queuing (aka Why isn't my job running?)"},{"location":"new_mod/","text":"New Module System \u00b6 In 2020 we started building packages and organising them into modules 1 in a new way. In the new system modules are organised into toolchains 2 . These toolchains were used to build the software. For most users the most important thing is these these toolchains act like software \"silos\". In general this restricts you to using using programs in one toolchain \"silo\". This is on the face of it, is annoying. However, it resolves some very hard to diagnose and subtle bugs that occur when you load programs that are built with different compilers - in the old system this was not transparent to you. Before you module load a toolchain the software contained within will not be visible to module loading (except via module spider). You first need to load the compiler and MPI version the software was built with. For example, if you wanted to load BioPython/1.79 you would first need to load GCC/10.3.0 and OpenMPI/4.1.1 To save you needing to load both a Compiler and MPI version, the compiler and MPI versions are bundled into half yearly packs. For example GCC/10.3.0 and OpenMPI/4.1.1 are bundled in the meta module foss/2021a graph TD; LMOD[\"Module System\"] --toolchain --- foss2020b[\"foss2020b GCC/10.2.0 OpenMPI/4.0.5\"] LMOD --toolchain --- foss2021a[\"foss2021a GCC/10.3.0 OpenMPI/4.1.1\"] foss2020b --- id3[\"ORCA/4.2.1\"] foss2020b --- id4[\"Singularity/3.7.3\"] foss2021a --- id5[\"Biopython/1.79\"] foss2021a --- id6[\"Haploflow/1.0\"] Example loading BioPython/1.7.9 # loading the modules module load foss/2021a # Needed for biopython to be loadable module load BioPython/1.7.9 Searching for Software with Spider \u00b6 As the software is siloed into toolchains methods for finding software like module avail are less useful than in the old system - it will only show software loadable in the current toolchain silo, or if no toolchain silos are loaded it'll show you all the toolchains as well as software that's not tied to a toolchain. We can use module spider to search for software modules more broadly - it will also show what toolchain needs to be loaded first. If we wanted to load a version of netCDF, we could search for it with spider. Note spider searches in a case sensitive manner - but it will suggest other capitalisation if other search options exist. module spider netCDF This returns a lot of information, but the important bit is: ------------- netCDF: ------------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. Versions: netCDF/4.7.1 netCDF/4.7.4 netCDF/4.8.0 -------------- For detailed information about a specific \"netCDF\" module ( including how to load the modules ) use the modules full name. For example: $ module spider netCDF/4.7.1 -------------- We have 3 versions of netCDF available in the new module system, 4.7.1 , 4.7.4 and 4.8.0 . To see which toolchain needs to be loaded, we spider that particular version module spider netCDF/4.7.1 #returns ---------- netCDF: netCDF/4.7.1 ---------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. You will need to load all module ( s ) on any one of the lines below before the \"netCDF/4.7.1\" module is available to load. GCC/8.3.0 OpenMPI/3.1.4 Help: Description =========== NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. More information ================ - Homepage: https://www.unidata.ucar.edu/software/netcdf/ This gives some information about the software as well as what we need to load it, in this case GCC/8.3.0 and OpenMPI/3.1.4 . We can just use that and load netcdf - #Load prerequisites - the \"silo\" module load GCC/8.3.0 module load OpenMPI/3.1.4 #Load NetCDF module load netCDF Alternatively you could load the toolchain containing gcc/8.3.0 and OpenMPI/3.1.4 - foss/2019b #Load prerequisites - the \"silo\" module load foss/2019b #Load NetCDF module load netCDF Toolchain \"silo\" table \u00b6 Toolchains currently on R\u0101poi as of May 2022. Toolchain Compiler MPI foss/2018b GCC/7.3.0 OpenMPI/3.1.1 foss/2019b GCC/8.3.0 OpenMPI/3.1.4 foss/2020a GCC/9.3.0 OpenMPI/4.0.3 foss/2020b GCC/10.2.0 OpenMPI/4.0.5 foss/2021a GCC/10.3.0 OpenMPI/4.1.1 There are also toolchain versions with CUDA for use on the GPU nodes - they contain the same compiler and OpenMPI but also include a CUDA version Toolchain Compiler MPI CUDA fosscuda/2019b GCC/8.3.0 OpenMPI/3.1.4 CUDA/10.1.243 fosscuda/2020b GCC/10.2.0 OpenMPI/4.0.5 CUDA/11.1.1 Lastly we have some intel compiler toolchains built. This might work on the AMD nodes, but you'll have an easier time with the intel nodes. Toolchain Compiler Intel Compiler MPI MKL intel/2021b GCC/11.2.0 2021.4.0 impi/2021.4.0 imkl/2021.4.0 intel/2022.00 GCC/11.2.0 2022.0.1 impi/2021.5.0 imkl/2022.0.1 You can also just experimentally module load the various toolchains and list to see what the module loads to see what it contains. #load toolchain module load foss/2020b # List what it loads module list # Returns Currently Loaded Modules: 1) config 6) libfabric/1.11.0 11) libxml2/2.9.10 16) FFTW/3.3.8 2) GCCcore/10.2.0 7) libevent/2.1.12 12) libpciaccess/0.16 17) OpenBLAS/0.3.12 3) binutils/2.35 8) numactl/2.0.13 13) hwloc/2.2.0 18) ScaLAPACK/2.1.0 4) GCC/10.2.0 9) XZ/5.2.5 14) PMIx/3.1.5 19) foss/2020b 5) UCX/1.9.0 10) zlib/1.2.11 15) OpenMPI/4.0.5 Add new module system for accounts prior to March 2022 \u00b6 Users accounts setup prior to March 2022 will not automatically have the new module system loaded. You can automatically use this new module system (in parallel with the old one) by adding a line to your .bashrc file. Users of zsh can make a similar change to their .zshrc file. Login to R\u0101poi and backup your .bashrc file, then edit it to add the needed line in. cd ~ # change to your home directory cp .bashrc .bashrc_backup #create backup of your bashrc file nano .bashrc #open the nano editor to edit your file #in nano find the line # module use -a /home/software/tools/modulefiles # it will be near the end of the file. After that line, add the line: module use /home/software/tools/eb_modulefiles/all/Core #press control-x to exit nano. It will ask if you want to save the modified buffer. Type Y to save the change. # It'll ask for the filename, just press enter to accept the name # .bashrc After you have made that change logout and log back in to have the new module system loaded. You can test it's working by loading a toolchain. module load foss/2020a If you run into problems, copy your .bashrc backup back to the original and try again with cp .bashrc_backup .bashrc Also feel free to ask for help on slack. Modules are the way programs are packaged up for you to use. We can't just install them system wide as we have hundreds of programs installed in modules, often with many versions of the same thing, they would conflict with each other. Modules let you load just what you need. See Preparing your environment \u21a9 The toolchain is comprised of a compiler and a version of MPI that was used to build the software. For instance the toolchain foss/2021a uses GCC/10.3.0 and OpenMPI/4.1.1 \u21a9","title":"New module system"},{"location":"new_mod/#new-module-system","text":"In 2020 we started building packages and organising them into modules 1 in a new way. In the new system modules are organised into toolchains 2 . These toolchains were used to build the software. For most users the most important thing is these these toolchains act like software \"silos\". In general this restricts you to using using programs in one toolchain \"silo\". This is on the face of it, is annoying. However, it resolves some very hard to diagnose and subtle bugs that occur when you load programs that are built with different compilers - in the old system this was not transparent to you. Before you module load a toolchain the software contained within will not be visible to module loading (except via module spider). You first need to load the compiler and MPI version the software was built with. For example, if you wanted to load BioPython/1.79 you would first need to load GCC/10.3.0 and OpenMPI/4.1.1 To save you needing to load both a Compiler and MPI version, the compiler and MPI versions are bundled into half yearly packs. For example GCC/10.3.0 and OpenMPI/4.1.1 are bundled in the meta module foss/2021a graph TD; LMOD[\"Module System\"] --toolchain --- foss2020b[\"foss2020b GCC/10.2.0 OpenMPI/4.0.5\"] LMOD --toolchain --- foss2021a[\"foss2021a GCC/10.3.0 OpenMPI/4.1.1\"] foss2020b --- id3[\"ORCA/4.2.1\"] foss2020b --- id4[\"Singularity/3.7.3\"] foss2021a --- id5[\"Biopython/1.79\"] foss2021a --- id6[\"Haploflow/1.0\"] Example loading BioPython/1.7.9 # loading the modules module load foss/2021a # Needed for biopython to be loadable module load BioPython/1.7.9","title":"New Module System"},{"location":"new_mod/#searching-for-software-with-spider","text":"As the software is siloed into toolchains methods for finding software like module avail are less useful than in the old system - it will only show software loadable in the current toolchain silo, or if no toolchain silos are loaded it'll show you all the toolchains as well as software that's not tied to a toolchain. We can use module spider to search for software modules more broadly - it will also show what toolchain needs to be loaded first. If we wanted to load a version of netCDF, we could search for it with spider. Note spider searches in a case sensitive manner - but it will suggest other capitalisation if other search options exist. module spider netCDF This returns a lot of information, but the important bit is: ------------- netCDF: ------------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. Versions: netCDF/4.7.1 netCDF/4.7.4 netCDF/4.8.0 -------------- For detailed information about a specific \"netCDF\" module ( including how to load the modules ) use the modules full name. For example: $ module spider netCDF/4.7.1 -------------- We have 3 versions of netCDF available in the new module system, 4.7.1 , 4.7.4 and 4.8.0 . To see which toolchain needs to be loaded, we spider that particular version module spider netCDF/4.7.1 #returns ---------- netCDF: netCDF/4.7.1 ---------- Description: NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. You will need to load all module ( s ) on any one of the lines below before the \"netCDF/4.7.1\" module is available to load. GCC/8.3.0 OpenMPI/3.1.4 Help: Description =========== NetCDF ( network Common Data Form ) is a set of software libraries and machine-independent data formats that support the creation, access, and sharing of array-oriented scientific data. More information ================ - Homepage: https://www.unidata.ucar.edu/software/netcdf/ This gives some information about the software as well as what we need to load it, in this case GCC/8.3.0 and OpenMPI/3.1.4 . We can just use that and load netcdf - #Load prerequisites - the \"silo\" module load GCC/8.3.0 module load OpenMPI/3.1.4 #Load NetCDF module load netCDF Alternatively you could load the toolchain containing gcc/8.3.0 and OpenMPI/3.1.4 - foss/2019b #Load prerequisites - the \"silo\" module load foss/2019b #Load NetCDF module load netCDF","title":"Searching for Software with Spider"},{"location":"new_mod/#toolchain-silo-table","text":"Toolchains currently on R\u0101poi as of May 2022. Toolchain Compiler MPI foss/2018b GCC/7.3.0 OpenMPI/3.1.1 foss/2019b GCC/8.3.0 OpenMPI/3.1.4 foss/2020a GCC/9.3.0 OpenMPI/4.0.3 foss/2020b GCC/10.2.0 OpenMPI/4.0.5 foss/2021a GCC/10.3.0 OpenMPI/4.1.1 There are also toolchain versions with CUDA for use on the GPU nodes - they contain the same compiler and OpenMPI but also include a CUDA version Toolchain Compiler MPI CUDA fosscuda/2019b GCC/8.3.0 OpenMPI/3.1.4 CUDA/10.1.243 fosscuda/2020b GCC/10.2.0 OpenMPI/4.0.5 CUDA/11.1.1 Lastly we have some intel compiler toolchains built. This might work on the AMD nodes, but you'll have an easier time with the intel nodes. Toolchain Compiler Intel Compiler MPI MKL intel/2021b GCC/11.2.0 2021.4.0 impi/2021.4.0 imkl/2021.4.0 intel/2022.00 GCC/11.2.0 2022.0.1 impi/2021.5.0 imkl/2022.0.1 You can also just experimentally module load the various toolchains and list to see what the module loads to see what it contains. #load toolchain module load foss/2020b # List what it loads module list # Returns Currently Loaded Modules: 1) config 6) libfabric/1.11.0 11) libxml2/2.9.10 16) FFTW/3.3.8 2) GCCcore/10.2.0 7) libevent/2.1.12 12) libpciaccess/0.16 17) OpenBLAS/0.3.12 3) binutils/2.35 8) numactl/2.0.13 13) hwloc/2.2.0 18) ScaLAPACK/2.1.0 4) GCC/10.2.0 9) XZ/5.2.5 14) PMIx/3.1.5 19) foss/2020b 5) UCX/1.9.0 10) zlib/1.2.11 15) OpenMPI/4.0.5","title":"Toolchain \"silo\" table"},{"location":"new_mod/#add-new-module-system-for-accounts-prior-to-march-2022","text":"Users accounts setup prior to March 2022 will not automatically have the new module system loaded. You can automatically use this new module system (in parallel with the old one) by adding a line to your .bashrc file. Users of zsh can make a similar change to their .zshrc file. Login to R\u0101poi and backup your .bashrc file, then edit it to add the needed line in. cd ~ # change to your home directory cp .bashrc .bashrc_backup #create backup of your bashrc file nano .bashrc #open the nano editor to edit your file #in nano find the line # module use -a /home/software/tools/modulefiles # it will be near the end of the file. After that line, add the line: module use /home/software/tools/eb_modulefiles/all/Core #press control-x to exit nano. It will ask if you want to save the modified buffer. Type Y to save the change. # It'll ask for the filename, just press enter to accept the name # .bashrc After you have made that change logout and log back in to have the new module system loaded. You can test it's working by loading a toolchain. module load foss/2020a If you run into problems, copy your .bashrc backup back to the original and try again with cp .bashrc_backup .bashrc Also feel free to ask for help on slack. Modules are the way programs are packaged up for you to use. We can't just install them system wide as we have hundreds of programs installed in modules, often with many versions of the same thing, they would conflict with each other. Modules let you load just what you need. See Preparing your environment \u21a9 The toolchain is comprised of a compiler and a version of MPI that was used to build the software. For instance the toolchain foss/2021a uses GCC/10.3.0 and OpenMPI/4.1.1 \u21a9","title":"Add new module system for accounts prior to March 2022"},{"location":"notebooks/","text":"Starting and Working with a Jupyter Notebook \u00b6 Step 1: The best way to start jupyter is with a batch submit script. We have created an example script. You can copy this script from one available on the cluster, just type the following: cp /home/software/vuwrc/examples/jupyter/notebook.sh notebook.sh If you are using Anaconda and have installed it in the default location you need to use the following submit file instead: cp /home/software/vuwrc/examples/jupyter/notebook-anaconda.sh notebook-anaconda.sh NOTE : We also have sbatch scripts for R (IRKernel)) notebooks. These are called R-notebook.sh and R-notebook-anaconda.sh This script is ready to run as is, but we recommend editing it to satisfy your own CPU, memory and time requirements. Once you have edited the file you can run it thusly: sbatch notebook.sh or if using Anaconda: sbatch notebook-anaconda.sh NOTE: If you copied the R notebook script, replace notebook.sh with R-notebook.sh This will submit the file to run a job. It may take some time for the job to run, depending on how busy the cluster is at the time. Once the job begins to run you will see some information in the file called notebook-JOBID.out (JOBID will be the actual jobid of this job, eg notebook-478903.out. If you view this file (users can type cat notebook-JOBID.out to view the file onscreen). You will see a line such as: The Jupyter Notebook is running at: http://130.195.19.20:47033/?token=SOME-RANDOM-HASH The 2 important pieces of information here are the IP address, in this case 130.195.19.20 and the port number, 47033 . These numbers should be different for you since the port number is random, although the IP Address may be the same since we have a limited number of compute nodes. Also notice after the ?token= you will see a random hash. This hash is a security feature and allows you to connect to the notebook. You will need to use these to view the notebook from your local machine. Step 2: To start working with the notebook you will need to tunnel a ssh session. In your SSH tunnel you will use the cluster login node (raapoi.vuw.ac.nz) to connect to the compute node (in the example above the compute node is at address 130.195.19.20) and transfer all the traffic back and forth between your computer and the compute node). Step 2a from a Mac: Open a new session window from Terminal.app or other terminal utility such as Xquartz and type the following: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 harrelwe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Step 2b: from Windows We recommend tunnelling using Git Bash, which is part of the Git for Windows project or MobaXTerm . There are 2 methods for tunneling in Moba, one is command line, the other is GUI-based. Method 1 (Git Bash or MobaXterm): Command-line, start a local Git Bash or MobaXterm terminal (or try the GUI method, below) From the command prompt type: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 harrelwe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Method 2 (MobaXterm): GUI-based, go to the Tunneling menu: Now click on New SSH Tunnel When you complete the creation of your tunnel click Start all tunnels . Enter your password and reply \"yes\" to any questions asked about accepting hostkeys or opening firewalls. You can safely exit the tunnel building menu. Step 3 Now open your favorite web browser and then use the URL from your job output file and paste it in your browsers location bar, for example my full URL was: http://130.195.19.20:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Step 4 One last thing you need to do is to replace the IP address with the word localhost . This will allow your browser to follow the tunnel you just opened and connect to the notebook running on an engaging compute node, in my case my address line will now look like this: http://localhost:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Now you can hit return and you should see your notebook running on an Engaging compute node. NOTE : when you are done with your notebook, please remember to cancel your job to free up the resources for others, hint: scancel If you want more information on working with Jupyter, there is good documentation, here: Jupyter Notebooks","title":"Using Jupyter Notebooks"},{"location":"notebooks/#starting-and-working-with-a-jupyter-notebook","text":"Step 1: The best way to start jupyter is with a batch submit script. We have created an example script. You can copy this script from one available on the cluster, just type the following: cp /home/software/vuwrc/examples/jupyter/notebook.sh notebook.sh If you are using Anaconda and have installed it in the default location you need to use the following submit file instead: cp /home/software/vuwrc/examples/jupyter/notebook-anaconda.sh notebook-anaconda.sh NOTE : We also have sbatch scripts for R (IRKernel)) notebooks. These are called R-notebook.sh and R-notebook-anaconda.sh This script is ready to run as is, but we recommend editing it to satisfy your own CPU, memory and time requirements. Once you have edited the file you can run it thusly: sbatch notebook.sh or if using Anaconda: sbatch notebook-anaconda.sh NOTE: If you copied the R notebook script, replace notebook.sh with R-notebook.sh This will submit the file to run a job. It may take some time for the job to run, depending on how busy the cluster is at the time. Once the job begins to run you will see some information in the file called notebook-JOBID.out (JOBID will be the actual jobid of this job, eg notebook-478903.out. If you view this file (users can type cat notebook-JOBID.out to view the file onscreen). You will see a line such as: The Jupyter Notebook is running at: http://130.195.19.20:47033/?token=SOME-RANDOM-HASH The 2 important pieces of information here are the IP address, in this case 130.195.19.20 and the port number, 47033 . These numbers should be different for you since the port number is random, although the IP Address may be the same since we have a limited number of compute nodes. Also notice after the ?token= you will see a random hash. This hash is a security feature and allows you to connect to the notebook. You will need to use these to view the notebook from your local machine. Step 2: To start working with the notebook you will need to tunnel a ssh session. In your SSH tunnel you will use the cluster login node (raapoi.vuw.ac.nz) to connect to the compute node (in the example above the compute node is at address 130.195.19.20) and transfer all the traffic back and forth between your computer and the compute node). Step 2a from a Mac: Open a new session window from Terminal.app or other terminal utility such as Xquartz and type the following: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 harrelwe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Step 2b: from Windows We recommend tunnelling using Git Bash, which is part of the Git for Windows project or MobaXTerm . There are 2 methods for tunneling in Moba, one is command line, the other is GUI-based. Method 1 (Git Bash or MobaXterm): Command-line, start a local Git Bash or MobaXterm terminal (or try the GUI method, below) From the command prompt type: ssh -L <PORT_NUMBER>:<IP_ADDRESS>:<PORT_NUMBER> username@raapoi.vuw.ac.nz For example: ssh -L 47033:130.195.19.20:47033 harrelwe@raapoi.vuw.ac.nz Once you are at a prompt you can go to Step 3 Method 2 (MobaXterm): GUI-based, go to the Tunneling menu: Now click on New SSH Tunnel When you complete the creation of your tunnel click Start all tunnels . Enter your password and reply \"yes\" to any questions asked about accepting hostkeys or opening firewalls. You can safely exit the tunnel building menu. Step 3 Now open your favorite web browser and then use the URL from your job output file and paste it in your browsers location bar, for example my full URL was: http://130.195.19.20:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Step 4 One last thing you need to do is to replace the IP address with the word localhost . This will allow your browser to follow the tunnel you just opened and connect to the notebook running on an engaging compute node, in my case my address line will now look like this: http://localhost:47033/?token=badef11b1371945b314e2e89b9a182f68e39dc40783ed68e Now you can hit return and you should see your notebook running on an Engaging compute node. NOTE : when you are done with your notebook, please remember to cancel your job to free up the resources for others, hint: scancel If you want more information on working with Jupyter, there is good documentation, here: Jupyter Notebooks","title":"Starting and Working with a Jupyter Notebook"},{"location":"parallel_processing/","text":"Parallel processing \u00b6 Running a job in parallel is a great way to utilize the power of the cluster. So what is a parallel job/workflow? Loosely-coupled jobs (sometimes referred to as embarrassingly or naively parallel jobs) are processes in which multiple instances of the same program execute on multiple data files simultaneously, with each instance running independently from others on its own allocated resources (i.e. CPUs and memory). Slurm job arrays offer a simple mechanism for achieving this. Multi-threaded programs that include explicit support for shared memory processing via multiple threads of execution (e.g. Posix Threads or OpenMP) running across multiple CPU cores. Distributed memory programs that include explicit support for message passing between processes (e.g. MPI). These processes execute across multiple CPU cores and/or nodes, these are often referred to as tightly-coupled jobs. GPU (graphics processing unit) programs including explicit support for offloading to the device via languages like CUDA or OpenCL. It is important to understand the capabilities and limitations of an application in order to fully leverage the parallel processing options available on the cluster. For instance, many popular scientific computing languages like Python, R, and Matlab now offer packages that allow for GPU, multi-core or multithreaded processing, especially for matrix and vector operations. If you need help with the design of your parallel workflow, send us a message in the raapoi-help Slack channel. Job Array Example \u00b6 Here is an example of running a job array to run 50 simultaneous processes: sbatch array.sh The contents of the array.sh batch script looks like this: #!/bin/bash #SBATCH -a 1-50 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=2G #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load fastqc/0.11.7 fastqc --nano -o $TMPDIR/output_dir seqfile_${SLURM_ARRAY_TASK_ID} So what do these parameter mean?: -a sets this up as a parallel array job (this sets up the \"loop\" that will be run --cpus-per-task requests the number of CPUs per array task, in this case I just want one CPU per task, we will use 50 in total --mem-per-cpu request 2GB of RAM per CPU, for this parallel job I will request a total of 100GB RAM (50 CPUs * 2GB RAM) --time is the max run time for this job, 10 minutes in this case --partition assigns this job to a partition module load fastqc/0.11.7 : Load software into my environment, in this case fastqc fastqc --nano -o $TMPDIR/output_dir seqfile ${SLURM_ARRAY_TASK_ID}_ Run fastqc on each input data file with the filenames seqfile_1, seqfile_2...seqfile_50 Running the array.sh script will cause the SLURM manager to spawn 50 parallel jobs. Multi-threaded or Multi-processing Job Example \u00b6 Multi-threaded or multi-processing programs are applications that are able to execute in parallel across multiple CPU cores within a single node using a shared memory execution model. In general, a multi-threaded application uses a single process (aka \u201ctask\u201d in Slurm) which then spawns multiple threads of execution. By default, Slurm allocates 1 CPU core per task. In order to make use of multiple CPU cores in a multi-threaded program, one must include the --cpus-per-task option. Below is an example of a multi-threaded program requesting 12 CPU cores per task and a total of 256GB of memory. The program itself is responsible for spawning the appropriate number of threads. #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=12 # 12 threads per task #SBATCH --time=02:00:00 # two hours #SBATCH --mem=256G #SBATCH -p bigmem #SBATCH --output=threaded.out #SBATCH --job-name=threaded #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com # Run multi-threaded application module load java/1.8.0-91 java -jar threaded-app.jar MPI Jobs \u00b6 Most users do not require MPI to run their jobs but many do. Please read on if you want to learn more about using MPI for tightly-coupled jobs. MPI (Message Passing Interface) code require special attention within Slurm. Slurm allocates and launches MPI jobs differently depending on the version of MPI used (e.g. OpenMPI, MPICH2). We recommend using OpenMPI version 2.1.1 or later to compile your C code (using mpicc) and then using the mpirun command in a batch submit script to launch parallel MPI jobs. The example below runs MPI code compiled by OpenMPI 2.1.1: #!/bin/bash #SBATCH --nodes=3 #SBATCH --tasks-per-node=8 # 8 MPI processes per node #SBATCH --time=3-00:00:00 #SBATCH --mem=4G # 4 GB RAM per node #SBATCH --output=mpi_job.log #SBATCH --partition=parallel #SBATCH --constraint=\"IB\" #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load openmpi echo $SLURM_JOB_NODELIST mpirun -np 24 mpiscript.o This example requests 3 nodes and 8 tasks (i.e. processes) per node, for a total of 24 tasks. I use this number to tell mpirun how many processes to start, -np 24 NOTE: We highly recomend adding the --constraint=\"IB\" parameter to your MPI job as this will ensure the job is run on nodes with an Infiniband interconnect. ALSO NOTE: If using python or another language you will also need to add the --oversubscribe parameter to mpirun, eg. mpirun --oversubscribe -np 24 mpiscript.py More information about running MPI jobs within Slurm can be found here here: http://slurm.schedmd.com/mpi_guide.html.","title":"Parallel Processing"},{"location":"parallel_processing/#parallel-processing","text":"Running a job in parallel is a great way to utilize the power of the cluster. So what is a parallel job/workflow? Loosely-coupled jobs (sometimes referred to as embarrassingly or naively parallel jobs) are processes in which multiple instances of the same program execute on multiple data files simultaneously, with each instance running independently from others on its own allocated resources (i.e. CPUs and memory). Slurm job arrays offer a simple mechanism for achieving this. Multi-threaded programs that include explicit support for shared memory processing via multiple threads of execution (e.g. Posix Threads or OpenMP) running across multiple CPU cores. Distributed memory programs that include explicit support for message passing between processes (e.g. MPI). These processes execute across multiple CPU cores and/or nodes, these are often referred to as tightly-coupled jobs. GPU (graphics processing unit) programs including explicit support for offloading to the device via languages like CUDA or OpenCL. It is important to understand the capabilities and limitations of an application in order to fully leverage the parallel processing options available on the cluster. For instance, many popular scientific computing languages like Python, R, and Matlab now offer packages that allow for GPU, multi-core or multithreaded processing, especially for matrix and vector operations. If you need help with the design of your parallel workflow, send us a message in the raapoi-help Slack channel.","title":"Parallel processing"},{"location":"parallel_processing/#job-array-example","text":"Here is an example of running a job array to run 50 simultaneous processes: sbatch array.sh The contents of the array.sh batch script looks like this: #!/bin/bash #SBATCH -a 1-50 #SBATCH --cpus-per-task=1 #SBATCH --mem-per-cpu=2G #SBATCH --time=00:10:00 #SBATCH --partition=parallel #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load fastqc/0.11.7 fastqc --nano -o $TMPDIR/output_dir seqfile_${SLURM_ARRAY_TASK_ID} So what do these parameter mean?: -a sets this up as a parallel array job (this sets up the \"loop\" that will be run --cpus-per-task requests the number of CPUs per array task, in this case I just want one CPU per task, we will use 50 in total --mem-per-cpu request 2GB of RAM per CPU, for this parallel job I will request a total of 100GB RAM (50 CPUs * 2GB RAM) --time is the max run time for this job, 10 minutes in this case --partition assigns this job to a partition module load fastqc/0.11.7 : Load software into my environment, in this case fastqc fastqc --nano -o $TMPDIR/output_dir seqfile ${SLURM_ARRAY_TASK_ID}_ Run fastqc on each input data file with the filenames seqfile_1, seqfile_2...seqfile_50 Running the array.sh script will cause the SLURM manager to spawn 50 parallel jobs.","title":"Job Array Example"},{"location":"parallel_processing/#multi-threaded-or-multi-processing-job-example","text":"Multi-threaded or multi-processing programs are applications that are able to execute in parallel across multiple CPU cores within a single node using a shared memory execution model. In general, a multi-threaded application uses a single process (aka \u201ctask\u201d in Slurm) which then spawns multiple threads of execution. By default, Slurm allocates 1 CPU core per task. In order to make use of multiple CPU cores in a multi-threaded program, one must include the --cpus-per-task option. Below is an example of a multi-threaded program requesting 12 CPU cores per task and a total of 256GB of memory. The program itself is responsible for spawning the appropriate number of threads. #!/bin/bash #SBATCH --nodes=1 #SBATCH --ntasks=1 #SBATCH --cpus-per-task=12 # 12 threads per task #SBATCH --time=02:00:00 # two hours #SBATCH --mem=256G #SBATCH -p bigmem #SBATCH --output=threaded.out #SBATCH --job-name=threaded #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com # Run multi-threaded application module load java/1.8.0-91 java -jar threaded-app.jar","title":"Multi-threaded or Multi-processing Job Example"},{"location":"parallel_processing/#mpi-jobs","text":"Most users do not require MPI to run their jobs but many do. Please read on if you want to learn more about using MPI for tightly-coupled jobs. MPI (Message Passing Interface) code require special attention within Slurm. Slurm allocates and launches MPI jobs differently depending on the version of MPI used (e.g. OpenMPI, MPICH2). We recommend using OpenMPI version 2.1.1 or later to compile your C code (using mpicc) and then using the mpirun command in a batch submit script to launch parallel MPI jobs. The example below runs MPI code compiled by OpenMPI 2.1.1: #!/bin/bash #SBATCH --nodes=3 #SBATCH --tasks-per-node=8 # 8 MPI processes per node #SBATCH --time=3-00:00:00 #SBATCH --mem=4G # 4 GB RAM per node #SBATCH --output=mpi_job.log #SBATCH --partition=parallel #SBATCH --constraint=\"IB\" #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load openmpi echo $SLURM_JOB_NODELIST mpirun -np 24 mpiscript.o This example requests 3 nodes and 8 tasks (i.e. processes) per node, for a total of 24 tasks. I use this number to tell mpirun how many processes to start, -np 24 NOTE: We highly recomend adding the --constraint=\"IB\" parameter to your MPI job as this will ensure the job is run on nodes with an Infiniband interconnect. ALSO NOTE: If using python or another language you will also need to add the --oversubscribe parameter to mpirun, eg. mpirun --oversubscribe -np 24 mpiscript.py More information about running MPI jobs within Slurm can be found here here: http://slurm.schedmd.com/mpi_guide.html.","title":"MPI Jobs"},{"location":"partitions/","text":"Partitions \u00b6 Using Partitions \u00b6 A partition is a collection of compute nodes, think of it as a sub-cluster or slice of the larger cluster. Each partition has its own rules and configurations. For example, the quicktest partition has a maximum job run-time of 5 hours, whereas the partition bigmem has a maximum runtime of 10 days. Partitions can also limit who can run a job. Currently any user can use any partition but there may come a time when certain research groups purchase their own nodes and they are given exclusive access. To view the partitions available to use you can type the vuw-partitions command, eg <user>@raapoi-master:~$ vuw-partitions VUW CLUSTER PARTITIONS PARTITION AVAIL TIMELIMIT NODES STATE NODELIST quicktest* up 5:00:00 4 idle itl04n[01-04] PARTITION AVAIL TIMELIMIT NODES STATE NODELIST gpu up 1-00:00:00 1 drain gpu01 gpu up 1-00:00:00 1 idle gpu02 PARTITION AVAIL TIMELIMIT NODES STATE NODELIST bigmem up 10-00:00:0 1 down* high05 bigmem up 10-00:00:0 1 mix high04 bigmem up 10-00:00:0 4 idle high[01-03,06] PARTITION AVAIL TIMELIMIT NODES STATE NODELIST parallel up 10-00:00:0 1 down* itl03n03 parallel up 10-00:00:0 1 drain amd01n04 parallel up 10-00:00:0 1 resv spj01 parallel up 10-00:00:0 19 mix amd01n[01-03],amd02n01,amd03n[01-02],amd05n[01,04],amd06n[01-04],itl01n[01-03],itl02n[01-02,04],itl03n01 parallel up 10-00:00:0 4 alloc itl01n04,itl02n03,itl03n[02,04] parallel up 10-00:00:0 10 idle amd02n[02-04],amd03n[03-04],amd04n[01-04],amd05n02 NOTE: This utility is a wrapper for the Slurm command: sinfo -p PARTITION Notice the STATE field, this describes the current condition of nodes within the partition, the most common states are defined as: idle - nodes in an idle state have no jobs running, all resources are available for work mix - nodes in a mixed state have some jobs running, but still have some resources available for work alloc - nodes in an alloc state are completely full, all resources are in use. drain - nodes in a drain state have some running jobs, but no new jobs can be run. This is typically done before the node goes into maintenance maint - node is in maintenance mode, no jobs can be submitted resv - node is in a reservation. A reservation is setup for future maintenance or for special purposes such as temporary dedicated access down - node is down, either for maitnenance or due to failure Also notice the TIMELIMIT field, this describes the maximum runtime of a partition. For example, the quicktest partition has a maximum runtime of 1 hour and the parallel partition has a max runtime of 10 days. Partition Descriptions \u00b6 Partition: quicktest \u00b6 This partition is for quick tests of code, environment, software builds or similar short-run jobs. Since the max time limit is 5 hours it should not take long for your job to run. This can also be used for near-on-demand interactive jobs. Quicktest nodes available: 4 Maximum CPU available per task: 48 Maximum memory available per task: 62G Maximum Runtime: 5 hours Partition: gpu \u00b6 This partition is for those jobs that require GPUs or those software that work with the CUDA platform and API (tensorflow, pytorch, MATLAB, etc) GPU nodes available: 2 GPUs available per node: 2 (A100's) Maximum CPU available per task: 256 Maximum memory available per task: 512G Maximum Runtime: 24 hours Note : To request GPU add the parameter, --gres=gpu:X Where X is the number of GPUs required, typically 1: --gres=gpu:1 - Partition: bigmem \u00b6 This partition is primarily useful for jobs that require very large shared memory (greater than 125 GB). These are known as memory-bound jobs. NOTE: Please do not schedule jobs of less than 125GB of memory on the bigmem partition. Bigmem nodes available: 5 (4x512G, 1x1000G) Maximum CPU available per task: 48 Maximum memory available per task: 1 TB (Note: maximum CPU for 1 TB is 40) Maximum Runtime: 10 days Partition: parallel \u00b6 This partition is useful for parallel workflows, either loosely coupled or jobs requiring MPI or other message passing protocols for tightly bound jobs. The total number of CPU's in this partition is 6816 with 2GB ram per CPU. AMD nodes - amdXXnXX AMD nodes available: 24 Maximum CPU available per task: 256 Maximum memory available per task: 512G Maximum Runtime: 10 days Intel nodes - itlXXnXX Intel nodes available: 12 Maximum CPU available per task: 64 Maximum memory available per task: 124G Maximum Runtime: 10 days Cluster Default Resources \u00b6 Please note that if you do not specify CPU, Memory or Time in your job request you will be given the cluster defaults which are: Default CPU: 2 Default Memory: 2 GB Default Time: 1 hour You can change these with the -c, --mem and --time parameters to the srun and sbatch commands. Please see this documentation for more information about srun and sbatch.","title":"Using Partitions"},{"location":"partitions/#partitions","text":"","title":"Partitions"},{"location":"partitions/#using-partitions","text":"A partition is a collection of compute nodes, think of it as a sub-cluster or slice of the larger cluster. Each partition has its own rules and configurations. For example, the quicktest partition has a maximum job run-time of 5 hours, whereas the partition bigmem has a maximum runtime of 10 days. Partitions can also limit who can run a job. Currently any user can use any partition but there may come a time when certain research groups purchase their own nodes and they are given exclusive access. To view the partitions available to use you can type the vuw-partitions command, eg <user>@raapoi-master:~$ vuw-partitions VUW CLUSTER PARTITIONS PARTITION AVAIL TIMELIMIT NODES STATE NODELIST quicktest* up 5:00:00 4 idle itl04n[01-04] PARTITION AVAIL TIMELIMIT NODES STATE NODELIST gpu up 1-00:00:00 1 drain gpu01 gpu up 1-00:00:00 1 idle gpu02 PARTITION AVAIL TIMELIMIT NODES STATE NODELIST bigmem up 10-00:00:0 1 down* high05 bigmem up 10-00:00:0 1 mix high04 bigmem up 10-00:00:0 4 idle high[01-03,06] PARTITION AVAIL TIMELIMIT NODES STATE NODELIST parallel up 10-00:00:0 1 down* itl03n03 parallel up 10-00:00:0 1 drain amd01n04 parallel up 10-00:00:0 1 resv spj01 parallel up 10-00:00:0 19 mix amd01n[01-03],amd02n01,amd03n[01-02],amd05n[01,04],amd06n[01-04],itl01n[01-03],itl02n[01-02,04],itl03n01 parallel up 10-00:00:0 4 alloc itl01n04,itl02n03,itl03n[02,04] parallel up 10-00:00:0 10 idle amd02n[02-04],amd03n[03-04],amd04n[01-04],amd05n02 NOTE: This utility is a wrapper for the Slurm command: sinfo -p PARTITION Notice the STATE field, this describes the current condition of nodes within the partition, the most common states are defined as: idle - nodes in an idle state have no jobs running, all resources are available for work mix - nodes in a mixed state have some jobs running, but still have some resources available for work alloc - nodes in an alloc state are completely full, all resources are in use. drain - nodes in a drain state have some running jobs, but no new jobs can be run. This is typically done before the node goes into maintenance maint - node is in maintenance mode, no jobs can be submitted resv - node is in a reservation. A reservation is setup for future maintenance or for special purposes such as temporary dedicated access down - node is down, either for maitnenance or due to failure Also notice the TIMELIMIT field, this describes the maximum runtime of a partition. For example, the quicktest partition has a maximum runtime of 1 hour and the parallel partition has a max runtime of 10 days.","title":"Using Partitions"},{"location":"partitions/#partition-descriptions","text":"","title":"Partition Descriptions"},{"location":"partitions/#partition-quicktest","text":"This partition is for quick tests of code, environment, software builds or similar short-run jobs. Since the max time limit is 5 hours it should not take long for your job to run. This can also be used for near-on-demand interactive jobs. Quicktest nodes available: 4 Maximum CPU available per task: 48 Maximum memory available per task: 62G Maximum Runtime: 5 hours","title":"Partition: quicktest"},{"location":"partitions/#partition-gpu","text":"This partition is for those jobs that require GPUs or those software that work with the CUDA platform and API (tensorflow, pytorch, MATLAB, etc) GPU nodes available: 2 GPUs available per node: 2 (A100's) Maximum CPU available per task: 256 Maximum memory available per task: 512G Maximum Runtime: 24 hours Note : To request GPU add the parameter, --gres=gpu:X Where X is the number of GPUs required, typically 1: --gres=gpu:1 -","title":"Partition: gpu"},{"location":"partitions/#partition-bigmem","text":"This partition is primarily useful for jobs that require very large shared memory (greater than 125 GB). These are known as memory-bound jobs. NOTE: Please do not schedule jobs of less than 125GB of memory on the bigmem partition. Bigmem nodes available: 5 (4x512G, 1x1000G) Maximum CPU available per task: 48 Maximum memory available per task: 1 TB (Note: maximum CPU for 1 TB is 40) Maximum Runtime: 10 days","title":"Partition: bigmem"},{"location":"partitions/#partition-parallel","text":"This partition is useful for parallel workflows, either loosely coupled or jobs requiring MPI or other message passing protocols for tightly bound jobs. The total number of CPU's in this partition is 6816 with 2GB ram per CPU. AMD nodes - amdXXnXX AMD nodes available: 24 Maximum CPU available per task: 256 Maximum memory available per task: 512G Maximum Runtime: 10 days Intel nodes - itlXXnXX Intel nodes available: 12 Maximum CPU available per task: 64 Maximum memory available per task: 124G Maximum Runtime: 10 days","title":"Partition: parallel"},{"location":"partitions/#cluster-default-resources","text":"Please note that if you do not specify CPU, Memory or Time in your job request you will be given the cluster defaults which are: Default CPU: 2 Default Memory: 2 GB Default Time: 1 hour You can change these with the -c, --mem and --time parameters to the srun and sbatch commands. Please see this documentation for more information about srun and sbatch.","title":"Cluster Default Resources"},{"location":"running_jobs/","text":"Running Jobs \u00b6 Job Basics \u00b6 R\u0101poi uses a scheduler and resource manager called Slurm that requires researchers to submit jobs for processing. There are 2 main types of jobs: batch and interactive. More details about submitting these types of jobs are below, but in general interactive jobs allow a user to interact with the application, for example a researcher can start a MATLAB session and can type MATLAB commands at a prompt or within a GUI. Batch jobs can work in the background and require no user interaction, they will start when resources are available and can be configured to email once a job completes. Job resources \u00b6 Jobs require resources. Basic resources are CPU, memory (aka RAM) and time. If the researcher does not specify the number of CPUs, RAM and time, the defaults will be given (currently 2CPU, 2 GB RAM and 1 hour of runtime.) Details on requesting the basic resources are included in the Batch and Interactive sections below. Along with basic resources there can be other resources defined, such as GPU, license tokens, or even specific types of CPUs and CPU instruction sets. Special resources can be requested using the parameters --gres or --constraint For example, to request an Intel processor one can use the parameter: --constraint=\"Intel\" Currently defined constraints \u00b6 Below is a list of constraints that have need defined and a brief description: AMD - AMD processor IB - Infiniband network for tightly coupled and MPI processing Intel - Intel processor 10GE - 10 Gigabit Ethernet SSE41 - Streaming SIMD Extensions version 4.1 AVX - Advanced Vector Extensions For example, if you want to request a compute node with AMD processors you can add --constraint=\"AMD\" in your submit script or srun request. Batch jobs \u00b6 To run a batch job (aka a job that runs unattended) you use the sbatch command. A simple example would look something like this: sbatch myjob.sh In this example the sbatch command runs the file myjob.sh, the contents of this file, also known as a \"batch submit script\" could look something like this: #!/bin/bash #SBATCH --cpus-per-task=2 #SBATCH --mem-per-cpu=2G #SBATCH --partition=parallel #SBATCH --time=3-12:00 #SBATCH -o /nfs/home/username/project1.out #SBATCH -e /nfs/home/username/project1.err #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load python/3.6.3 python3 project1.py This will request 2 CPUs and 4GB of memory (2GB per CPU) and a runtime of 3 days 12 hours. We are requesting that this job be run on the parallel partition, it will then load the environment module for python version 3.6.3 and run a python script called project1.py. Any output from the script will be placed in your home directory in a file named project1.out and any error information in a file called project1.err. If you do not specify an output or error file, the default files will have the form of Slurm-jobID.o and Slurm-jobID.e and will be located in the directory from which you ran sbatch . NOTE: We have this example script available to copy on the cluster, you can type the following to copy it to your home directory: cp /home/software/tools/examples/batch/myjob.sh ~/myjob.sh The ~/ in front of the file is a short-cut to your home directory path. You will want to edit this file accordingly. For more information on the sbatch command, please use the manpages, eg: man sbatch Interactive jobs \u00b6 One of the basic job submittal tools is the command srun For example, say I want to start a job to run an interactive R session. Once logged into the cluster I can: module load R/3.5.1 srun --pty --cpus-per-task=2 --mem=2G --time=08:00:00 --partition=quicktest R So what does this all mean? The module load command will introduce the environment necessary to run a particular program, in this case R version 3.5.1 The srun command will submit the job to the cluster. The srun command has many parameter available, some of the most common are in this example and explained below --pty - Required to run interactively --cpus-per-task=2 - requests 2 CPUs, can also use the -c flag, eg. -c 2 --mem=2G - requests 2 GigaBytes (GB) of RAM. --time=08:00:00 - requests a runtime of up to 8 hours (format is DAYS-HOURS:MINUTES:SECONDS), this is important in case the cluster or partition has a limited run-time, for example if an outage window is approaching. Keep in mind time is a resource along with CPU and Memory. --partition=quicktest - requests a certain partition, in this case it requests the quicktest partition, see the section on using cluster partitions for more information. R - the command you wish to run, this could also be matlab, python, etc. (just remember to load the module first) For more information on the srun command, please use the manpages, eg: man srun","title":"Running Jobs"},{"location":"running_jobs/#running-jobs","text":"","title":"Running Jobs"},{"location":"running_jobs/#job-basics","text":"R\u0101poi uses a scheduler and resource manager called Slurm that requires researchers to submit jobs for processing. There are 2 main types of jobs: batch and interactive. More details about submitting these types of jobs are below, but in general interactive jobs allow a user to interact with the application, for example a researcher can start a MATLAB session and can type MATLAB commands at a prompt or within a GUI. Batch jobs can work in the background and require no user interaction, they will start when resources are available and can be configured to email once a job completes.","title":"Job Basics"},{"location":"running_jobs/#job-resources","text":"Jobs require resources. Basic resources are CPU, memory (aka RAM) and time. If the researcher does not specify the number of CPUs, RAM and time, the defaults will be given (currently 2CPU, 2 GB RAM and 1 hour of runtime.) Details on requesting the basic resources are included in the Batch and Interactive sections below. Along with basic resources there can be other resources defined, such as GPU, license tokens, or even specific types of CPUs and CPU instruction sets. Special resources can be requested using the parameters --gres or --constraint For example, to request an Intel processor one can use the parameter: --constraint=\"Intel\"","title":"Job resources"},{"location":"running_jobs/#currently-defined-constraints","text":"Below is a list of constraints that have need defined and a brief description: AMD - AMD processor IB - Infiniband network for tightly coupled and MPI processing Intel - Intel processor 10GE - 10 Gigabit Ethernet SSE41 - Streaming SIMD Extensions version 4.1 AVX - Advanced Vector Extensions For example, if you want to request a compute node with AMD processors you can add --constraint=\"AMD\" in your submit script or srun request.","title":"Currently defined constraints"},{"location":"running_jobs/#batch-jobs","text":"To run a batch job (aka a job that runs unattended) you use the sbatch command. A simple example would look something like this: sbatch myjob.sh In this example the sbatch command runs the file myjob.sh, the contents of this file, also known as a \"batch submit script\" could look something like this: #!/bin/bash #SBATCH --cpus-per-task=2 #SBATCH --mem-per-cpu=2G #SBATCH --partition=parallel #SBATCH --time=3-12:00 #SBATCH -o /nfs/home/username/project1.out #SBATCH -e /nfs/home/username/project1.err #SBATCH --mail-type=BEGIN,END,FAIL #SBATCH --mail-user=me@email.com module load python/3.6.3 python3 project1.py This will request 2 CPUs and 4GB of memory (2GB per CPU) and a runtime of 3 days 12 hours. We are requesting that this job be run on the parallel partition, it will then load the environment module for python version 3.6.3 and run a python script called project1.py. Any output from the script will be placed in your home directory in a file named project1.out and any error information in a file called project1.err. If you do not specify an output or error file, the default files will have the form of Slurm-jobID.o and Slurm-jobID.e and will be located in the directory from which you ran sbatch . NOTE: We have this example script available to copy on the cluster, you can type the following to copy it to your home directory: cp /home/software/tools/examples/batch/myjob.sh ~/myjob.sh The ~/ in front of the file is a short-cut to your home directory path. You will want to edit this file accordingly. For more information on the sbatch command, please use the manpages, eg: man sbatch","title":"Batch jobs"},{"location":"running_jobs/#interactive-jobs","text":"One of the basic job submittal tools is the command srun For example, say I want to start a job to run an interactive R session. Once logged into the cluster I can: module load R/3.5.1 srun --pty --cpus-per-task=2 --mem=2G --time=08:00:00 --partition=quicktest R So what does this all mean? The module load command will introduce the environment necessary to run a particular program, in this case R version 3.5.1 The srun command will submit the job to the cluster. The srun command has many parameter available, some of the most common are in this example and explained below --pty - Required to run interactively --cpus-per-task=2 - requests 2 CPUs, can also use the -c flag, eg. -c 2 --mem=2G - requests 2 GigaBytes (GB) of RAM. --time=08:00:00 - requests a runtime of up to 8 hours (format is DAYS-HOURS:MINUTES:SECONDS), this is important in case the cluster or partition has a limited run-time, for example if an outage window is approaching. Keep in mind time is a resource along with CPU and Memory. --partition=quicktest - requests a certain partition, in this case it requests the quicktest partition, see the section on using cluster partitions for more information. R - the command you wish to run, this could also be matlab, python, etc. (just remember to load the module first) For more information on the srun command, please use the manpages, eg: man srun","title":"Interactive jobs"},{"location":"storage/","text":"Storage and quotas \u00b6 Shared Storage \u00b6 Currently users have 3 main storage areas share across every node. Each node has access to to this storage at all times and data is shared. Be careful with parallel jobs trying to write to the same filename! /nfs/home/USERNAME - This is your Home Directory, each user has a 50 GB quota limit. The data is replicated off site and backed up regularly by Digital Solutions. Home directory tips /nfs/scratch/USERNAME - This is your scratch space, each user has a 5 TB quota limit. This data is not backed up! Scratch directory tips /beegfs-volatile/USERNAME - This is fast parallel filesystem. There is no quota enforcement here. There is 100TB of total space. This data is not backed up! All data on this storage is periodically deleted BeeGFS tips Note: Home directory quotas cannot be increased, however if you need more space in your scratch folder let us know. To view your current quota and usage use the vuw-quota command, for example: <username@raapoi-master:~$ vuw-quota User Quotas Storage Usage ( GB ) Quota ( GB ) % Used /nfs/home/<username> 18 .32 50 .00 36 .63% /nfs/scratch/<username> 0 .00 5000 .00 0 .00% Per Node Storage \u00b6 Each node has local storage you can use at /tmp . This storage is not shared so a program running on amd01n02 will not be able to see data stored on node amd01n04 's /tmp storage. On the AMD nodes and GPU nodes this is very fast nvme storage with 1.7TB total space. On the Intel and highmem nodes this storage is slower and 1.7TB is not always available. If you use this storage it is your responsibility to copy data to the /tmp and clean it up when your job is done. For more info see Temp Disk Tips . Storage Performance \u00b6 graph TD A(Home and Research Storage) --> B B[Scratch] --> C C[BeeGFS] --> D D[local tmp on AMD nodes] Figure 1: Storage speed hierarchy. The slowest storage is your user home directory as well as any mounted research storage. The trade off for this is that this data is replicated off site as well as backed up by Digital Solutions. The fastest is the local tmp space on the AMD nodes - it is usually deleted shortly after you logout and only visible to the node it's on, but it is extremely fast with excellent IO performance.","title":"Storage and Quotas"},{"location":"storage/#storage-and-quotas","text":"","title":"Storage and quotas"},{"location":"storage/#shared-storage","text":"Currently users have 3 main storage areas share across every node. Each node has access to to this storage at all times and data is shared. Be careful with parallel jobs trying to write to the same filename! /nfs/home/USERNAME - This is your Home Directory, each user has a 50 GB quota limit. The data is replicated off site and backed up regularly by Digital Solutions. Home directory tips /nfs/scratch/USERNAME - This is your scratch space, each user has a 5 TB quota limit. This data is not backed up! Scratch directory tips /beegfs-volatile/USERNAME - This is fast parallel filesystem. There is no quota enforcement here. There is 100TB of total space. This data is not backed up! All data on this storage is periodically deleted BeeGFS tips Note: Home directory quotas cannot be increased, however if you need more space in your scratch folder let us know. To view your current quota and usage use the vuw-quota command, for example: <username@raapoi-master:~$ vuw-quota User Quotas Storage Usage ( GB ) Quota ( GB ) % Used /nfs/home/<username> 18 .32 50 .00 36 .63% /nfs/scratch/<username> 0 .00 5000 .00 0 .00%","title":"Shared Storage"},{"location":"storage/#per-node-storage","text":"Each node has local storage you can use at /tmp . This storage is not shared so a program running on amd01n02 will not be able to see data stored on node amd01n04 's /tmp storage. On the AMD nodes and GPU nodes this is very fast nvme storage with 1.7TB total space. On the Intel and highmem nodes this storage is slower and 1.7TB is not always available. If you use this storage it is your responsibility to copy data to the /tmp and clean it up when your job is done. For more info see Temp Disk Tips .","title":"Per Node Storage"},{"location":"storage/#storage-performance","text":"graph TD A(Home and Research Storage) --> B B[Scratch] --> C C[BeeGFS] --> D D[local tmp on AMD nodes] Figure 1: Storage speed hierarchy. The slowest storage is your user home directory as well as any mounted research storage. The trade off for this is that this data is replicated off site as well as backed up by Digital Solutions. The fastest is the local tmp space on the AMD nodes - it is usually deleted shortly after you logout and only visible to the node it's on, but it is extremely fast with excellent IO performance.","title":"Storage Performance"},{"location":"support/","text":"Check out the Documentation section for a comprehensive overview of how to use R\u0101poi. Visit the R\u0101poi Slack Channel for help, software requests or to communicate with others in the R\u0101poi community. Need an account on R\u0101poi or access to the Slack channel? Contact the CAD Research Support team: Andre Geldenhuis Research Software Engineer andre.geldenhuis@vuw.ac.nz Matt Plummer Digital Research Consultant matt.plummer@vuw.ac.nz","title":"Support"},{"location":"training/","text":"Training \u00b6 These are longer worked examples. If you have domain specific training you'd like to provide for your students or peers, contact Andre, or make a pull request against this repo. GPU example with neural style in pytorch \u00b6 We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo. Clone the pytorch example repo \u00b6 In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running. Load the modules \u00b6 We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6 Optional: Setup a virtualenv \u00b6 python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages. Download some images to use as content as well as for training. \u00b6 In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py Style some images - inference \u00b6 We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram. Train a new style - computationally expensive. \u00b6 Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm. Use our newly trained network \u00b6 submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1 Bonus content use a slurm task-array to find the optimum parameters. \u00b6 In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1 Simple OpenMPI with Singularity using the hybrid approach. \u00b6 The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem-per-cpu=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done Simple tensorflow example (using new module system) \u00b6 In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible errors - Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc /var/lib/slurm/slurmd/job1125851/slurm_script: line 21: 46983 Aborted (core dumped) python example.py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples In the tensorflow-simple directory","title":"Training"},{"location":"training/#training","text":"These are longer worked examples. If you have domain specific training you'd like to provide for your students or peers, contact Andre, or make a pull request against this repo.","title":"Training"},{"location":"training/#gpu-example-with-neural-style-in-pytorch","text":"We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo.","title":"GPU example with neural style in pytorch"},{"location":"training/#clone-the-pytorch-example-repo","text":"In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running.","title":"Clone the pytorch example repo"},{"location":"training/#load-the-modules","text":"We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6","title":"Load the modules"},{"location":"training/#optional-setup-a-virtualenv","text":"python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages.","title":"Optional: Setup a virtualenv"},{"location":"training/#download-some-images-to-use-as-content-as-well-as-for-training","text":"In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py","title":"Download some images to use as content as well as for training."},{"location":"training/#style-some-images-inference","text":"We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram.","title":"Style some images - inference"},{"location":"training/#train-a-new-style-computationally-expensive","text":"Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm.","title":"Train a new style - computationally expensive."},{"location":"training/#use-our-newly-trained-network","text":"submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1","title":"Use our newly trained network"},{"location":"training/#bonus-content-use-a-slurm-task-array-to-find-the-optimum-parameters","text":"In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1","title":"Bonus content use a slurm task-array to find the optimum parameters."},{"location":"training/#simple-openmpi-with-singularity-using-the-hybrid-approach","text":"The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem-per-cpu=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done","title":"Simple OpenMPI with Singularity using the hybrid approach."},{"location":"training/#simple-tensorflow-example-using-new-module-system","text":"In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible errors - Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc /var/lib/slurm/slurmd/job1125851/slurm_script: line 21: 46983 Aborted (core dumped) python example.py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples In the tensorflow-simple directory","title":"Simple tensorflow example (using new module system)"},{"location":"usersub/","text":"User Submitted Documentation \u00b6 This is user submitted documentation. This will eventually contain tip and tricks from users. VPN alternatives \u00b6 While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems. ECS ssh bastions \u00b6 ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user> OpenConnect \u00b6 OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi.","title":"User Submitted Docs"},{"location":"usersub/#user-submitted-documentation","text":"This is user submitted documentation. This will eventually contain tip and tricks from users.","title":"User Submitted Documentation"},{"location":"usersub/#vpn-alternatives","text":"While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems.","title":"VPN alternatives"},{"location":"usersub/#ecs-ssh-bastions","text":"ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user>","title":"ECS ssh bastions"},{"location":"usersub/#openconnect","text":"OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi.","title":"OpenConnect"},{"location":"examples/anaconda/","text":"Using Anaconda/Miniconda/conda - idba \u00b6 Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is available if you want a more minimal initial setup. module load old-mod-system/Anaconda3/2020.11 Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct conda activate idba-example #activate our example environment. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/andre/anaconda3 idba-example /home/andre/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load old-mod-system/Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/andre/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Anaconda"},{"location":"examples/anaconda/#using-anacondaminicondaconda-idba","text":"Many users use Anaconda/Miniconda to manage software stacks. One way to do this is to use singularity containers with the conda environment inside - this allows the conda environment to load quickly as the many small conda files are inside a container which the file system sees as one file. However, this is also an additional bit of complexity so many users just use conda outside of singularity. You can install your own version of Anaconda/Miniconda to your home directory or scratch. We have also got packaged versions of Anaconda/Miniconda installed with our module loading system. Anaconda has many built in packages so we will use that in our examples, but Miniconda is available if you want a more minimal initial setup. module load old-mod-system/Anaconda3/2020.11 Let's create a new conda environment for this example, in a sensible location, I used ~/examples/conda/idba conda create --name idba-example # press y for the Proceed prompt if it looks correct conda activate idba-example #activate our example environment. Conda environments are beyond the scope of this example, but they are a good way to contain all the dependencies and programs for a particular workflow, in this case, idba. Install idba in our conda environment. conda install -c bioconda idba Idba is a genome assembler, we will use paired-end illumina reads of E. coli. The data is available on an Amazon S3 bucket (a cloud storage location), and we can download it using wget. mkdir data # put our data in a sensible location cd data wget --content-disposition goo.gl/JDJTaz #sequence data wget --content-disposition goo.gl/tt9fsn #sequence data cd .. #back to our project directory The reads we have are paired-end fastq files but idba requires a fasta file. We can use a tool installed with idba to convert them. We'll do this on the R\u0101poi login node as it is a fast task that doesn't need many resources. fq2fa --merge --filter data/MiSeq_Ecoli_MG1655_50x_R1.fastq data/MiSeq_Ecoli_MG1655_50x_R2.fastq data/read.fa To create our submission script we need to know the path to our conda enviroment. To get this: conda env list You'll need to find your idba-example environment, and next to it is the path you'll need for your submission script. In my case: # conda environments: # base * /home/andre/anaconda3 idba-example /home/andre/anaconda3/envs/idba-example # We need this line, it'll be different for you! Create our sbatch submission script. Note that this sequence doesn't need a lot of memory, so we'll use 3G. To see your usage after the job has run use vuw-job-report <job-id> idba_submit.sh #!/bin/bash #SBATCH --job-name=idba_test #SBATCH -o _output.out #SBATCH -e _output.err #SBATCH --time=00:5:00 #SBATCH --partition=quicktest #SBATCH --ntasks=12 #SBATCH --mem=3G module load old-mod-system/Anaconda3/2020.11 eval \" $( conda shell.bash hook ) \" # basically inits your conda - prevents errors like: CommandNotFoundError: Your shell has not been properly configured ... conda activate /home/andre/anaconda3/envs/idba-example # We will need to activate our conda enviroment on the remote node idba idba_ud -r data/read.fa -o output To submit our job sbatch idba_submit.sh To see our job running or queuing squeue -u $USER This job will take a few minutes to run, generally less than 5. When the job is done we can see the output in the output folder. We can also see the std output and std err in the files _output.out and _output.err . The quickest way to examine them is to cat the files when the run is done. cat _output.out","title":"Using Anaconda/Miniconda/conda - idba"},{"location":"storage/beegfs/","text":"BeeGFS Tips \u00b6 The BeeGFS storage is spread across 3 nodes with SSD disks. The aggregate storage is 100TB. We don't enforce quotas here as some projects have large storage needs. However, you do need to be a good HPC citizen and respect the rights of others. Don't needlessly fill up this storage. We regularly delete all data here every 3-6 months. We only post warnings on the slack channel . If afteer 3 months the storage is not full and still performning well, we delay the wipe another 3 months. This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. BeeGFS should have better IO performance than the scratch storage - however it does depend what other users are doing. No filesystem likes small files, BeeGFS likes small files even less than scratch. Filesizes over 1MB are best. If you have a large amount of files you can improve performance by splitting your files accross many directories - the load balancing accross the metadata and storage servers is by directory, not file.","title":"Beegfs"},{"location":"storage/beegfs/#beegfs-tips","text":"The BeeGFS storage is spread across 3 nodes with SSD disks. The aggregate storage is 100TB. We don't enforce quotas here as some projects have large storage needs. However, you do need to be a good HPC citizen and respect the rights of others. Don't needlessly fill up this storage. We regularly delete all data here every 3-6 months. We only post warnings on the slack channel . If afteer 3 months the storage is not full and still performning well, we delay the wipe another 3 months. This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. BeeGFS should have better IO performance than the scratch storage - however it does depend what other users are doing. No filesystem likes small files, BeeGFS likes small files even less than scratch. Filesizes over 1MB are best. If you have a large amount of files you can improve performance by splitting your files accross many directories - the load balancing accross the metadata and storage servers is by directory, not file.","title":"BeeGFS Tips"},{"location":"storage/home/","text":"Home Directory Tips \u00b6 Home directories have a small quota and are on fairly slow storage. The data here is backed up . It is replicated off site live as well as periodically backed up to off site tape. In theory data here is fairly safe, even in a catastophic event it should be recoverable eventually. If you accidentally delete something here it can be recovered with a service desk request. While this storage is not performant, is is quite safe and is a good place for you scripts and code to live. Your data sets can also be on your home if they fit and the performance doesn't cause you any problems. For bigger or faster storage see the Scratch or BeeGFS pages.","title":"Home"},{"location":"storage/home/#home-directory-tips","text":"Home directories have a small quota and are on fairly slow storage. The data here is backed up . It is replicated off site live as well as periodically backed up to off site tape. In theory data here is fairly safe, even in a catastophic event it should be recoverable eventually. If you accidentally delete something here it can be recovered with a service desk request. While this storage is not performant, is is quite safe and is a good place for you scripts and code to live. Your data sets can also be on your home if they fit and the performance doesn't cause you any problems. For bigger or faster storage see the Scratch or BeeGFS pages.","title":"Home Directory Tips"},{"location":"storage/scratch/","text":"Scratch Tips \u00b6 The scratch storage is on a large storage node with 2 raid arrays with 50TB of storage each. Your scratch will always be available at /nfs/scratch/<username> . Your scratch storage could be on scratch or scratch2, to find out run vuw-quota . Each user has a quota of 5TB on scratch - you can ask us to increase it if needed. While each user has a quota of 5TB, we don't actually have enough storage for each user to fill 5TB of storage! This is a shared resource and we will occationally ask on the slack channel for users to clean up their storage to make space for others. To check how much space is free on the scratch storage for all users, on R\u0101poi: df -h | grep scratch #df -h is disk free with human units, | pipes the output to grep, which shows lines which contain the word scratch On the slack channel in any DM or channel type /df-scratch The output should only be visible to you This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. Scratch is also not a place for your old data to live forever, please clean up datasets you're no longer using!","title":"Scratch"},{"location":"storage/scratch/#scratch-tips","text":"The scratch storage is on a large storage node with 2 raid arrays with 50TB of storage each. Your scratch will always be available at /nfs/scratch/<username> . Your scratch storage could be on scratch or scratch2, to find out run vuw-quota . Each user has a quota of 5TB on scratch - you can ask us to increase it if needed. While each user has a quota of 5TB, we don't actually have enough storage for each user to fill 5TB of storage! This is a shared resource and we will occationally ask on the slack channel for users to clean up their storage to make space for others. To check how much space is free on the scratch storage for all users, on R\u0101poi: df -h | grep scratch #df -h is disk free with human units, | pipes the output to grep, which shows lines which contain the word scratch On the slack channel in any DM or channel type /df-scratch The output should only be visible to you This storage is not backed up at all. It is on a raid array so if a hard drive fails your data is safe. However in the event of a more dramatic hardware failure, earthquakes or fire - your data is gone forever. If you accidentally delete something, it's gone forever. If an Admin misconfigures something, your data is gone (we try not to do this!). It is your responsiblilty to backup your data here - a good place is to use Digital Solutions Research Storage. Scratch is also not a place for your old data to live forever, please clean up datasets you're no longer using!","title":"Scratch Tips"},{"location":"storage/tmp/","text":"Temp Disk Tips \u00b6 This storage is very fast on the AMD nodes and GPU nodes. It is your job to move data to the tmp space and clean it up when done. There is very little management of this space and currently it is not visible to slurm for fair use scheduling - in other words someone else might have used up most of the temp space on the node! This is generally not the case though. A rough example of how you could use this in an sbatch script #!/bin/bash # #SBATCH --job-name=bash_test # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --time=10:00 # Do the needed module loading for your use case module load etc #make a temporary directory with your usename so you don't tread on others mkdir /tmp/<username> #Copy dataset from scratch to the local tmp on the node (could also use rsync) cp -r /nfs/scratch/<user>/dataset /tmp/<user>/dataset Process data against /tmp/<user>/dataset Lets say the data is output to /tmp/<user>/dataoutput/ # Copy data from output to your scratch - I suggest not overwriting your original dataset! cp -r /tmp/<user>/dataoutput/* /nfs/scratch/<user>/dataset/dataoutput/ # Delete the data you copy to and created on tmp rm -rf /tmp/<user> #DANGER!!","title":"Tmp"},{"location":"storage/tmp/#temp-disk-tips","text":"This storage is very fast on the AMD nodes and GPU nodes. It is your job to move data to the tmp space and clean it up when done. There is very little management of this space and currently it is not visible to slurm for fair use scheduling - in other words someone else might have used up most of the temp space on the node! This is generally not the case though. A rough example of how you could use this in an sbatch script #!/bin/bash # #SBATCH --job-name=bash_test # #SBATCH --partition=quicktest # #SBATCH --cpus-per-task=1 #SBATCH --mem=1G #SBATCH --time=10:00 # Do the needed module loading for your use case module load etc #make a temporary directory with your usename so you don't tread on others mkdir /tmp/<username> #Copy dataset from scratch to the local tmp on the node (could also use rsync) cp -r /nfs/scratch/<user>/dataset /tmp/<user>/dataset Process data against /tmp/<user>/dataset Lets say the data is output to /tmp/<user>/dataoutput/ # Copy data from output to your scratch - I suggest not overwriting your original dataset! cp -r /tmp/<user>/dataoutput/* /nfs/scratch/<user>/dataset/dataoutput/ # Delete the data you copy to and created on tmp rm -rf /tmp/<user> #DANGER!!","title":"Temp Disk Tips"},{"location":"training/gpu-neural-style/","text":"GPU example with neural style in pytorch \u00b6 We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo. Clone the pytorch example repo \u00b6 In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running. Load the modules \u00b6 We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6 Optional: Setup a virtualenv \u00b6 python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages. Download some images to use as content as well as for training. \u00b6 In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py Style some images - inference \u00b6 We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram. Train a new style - computationally expensive. \u00b6 Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm. Use our newly trained network \u00b6 submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1 Bonus content use a slurm task-array to find the optimum parameters. \u00b6 In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1","title":"Gpu neural style"},{"location":"training/gpu-neural-style/#gpu-example-with-neural-style-in-pytorch","text":"We'll do a quick python example using neural style implemented in pytorch. We will be using modules rather than conda/virtualenvs but there is nothing stopping you from loading the modules and creating a virtualenv/conda enviroment to install additional python packages. The code we use will come from the pytorch example git repo.","title":"GPU example with neural style in pytorch"},{"location":"training/gpu-neural-style/#clone-the-pytorch-example-repo","text":"In a sensible location, clone the rep. git clone https://github.com/pytorch/examples.git cd examples/fast_neural_style # change to the example we will be running.","title":"Clone the pytorch example repo"},{"location":"training/gpu-neural-style/#load-the-modules","text":"We are using the new Easybuild based modules, to ensure we don't have conflicts with the old modules, it will be best to unuse them first and then use the new system. At somepoint we may automatically add the new modules to your bashrc file - but currently you'll have to do this yourself or manually unuse and use the new module system. module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 module list #see all the dependencies we have loaded, in particular which version of python we're using now. Currently Python 3.8.6","title":"Load the modules"},{"location":"training/gpu-neural-style/#optional-setup-a-virtualenv","text":"python3 -m venv env # create a virtualenv folder called env. Note! This will likely only work with the python version listed above! source env/bin/activate # activate the virtualenv Now that we've activated the virtual environment, we can install any additional packages we need. In this case we don't need any additional packages.","title":"Optional: Setup a virtualenv"},{"location":"training/gpu-neural-style/#download-some-images-to-use-as-content-as-well-as-for-training","text":"In your examples/fast_neural_style/ directory. # Download an image of an octopus to images/content-images. ## CC BY-SA 3.0 H. Zell wget https://upload.wikimedia.org/wikipedia/commons/0/0c/Octopus_vulgaris_02.JPG -P images/content-images/ # Download an image of The Great Wave off Kanagawa - public domain wget https://upload.wikimedia.org/wikipedia/commons/a/a5/Tsunami_by_hokusai_19th_century.jpg -O images/style-images/wave.jpg Depending on the GPU we are using, we may need to resize the image to ensure it fits in memory. On an RTX6000 we would need to resize the image to 70% of its full size to fit in memroy. Thankfully the GPUs on R\u0101poi are A100's with 40GB of ram, so we can skip this step. We will also need to download the pre-trained models for our initial inference runs. python download_saved_models.py","title":"Download some images to use as content as well as for training."},{"location":"training/gpu-neural-style/#style-some-images-inference","text":"We'll initially just use pretrained models to generate styled images - this is known as model inference and is much less intensive than training the model, we'll do this on both CPU and GPU. submit_cpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=parallel #SBATCH --ntasks=12 #SBATCH --mem=6G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 0 means run on the CPU and we'll save the output image as test1.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test1.jpg --cuda 0 You can check how long the job took to run with vuw-job-history . The last lines are your last run job, in my case: 332281 COMPLETED pytorch_t+ 00 :02:36 332281 .batch COMPLETED batch 0 .15G 00 :02:36 332281 .exte+ COMPLETED extern 0 .15G 00 :02:36 the job took 2:36. Let's run the inference job again on GPU to see the speedup. submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval --content-image images/content-images/Octopus_vulgaris_02.JPG --model saved_models/mosaic.pth --output-image ./test2.jpg --cuda 1 In this case vuw-job-history the job took: 692973 COMPLETED pytorch_t+ 00 :00:16 692973 .batch COMPLETED batch 0 .15G 00 :00:16 692973 .exte+ COMPLETED extern 0 .15G 00 :00:16 but the time varies a lot with short GPU runs, some are nearly 2 min long and some runs are 16s with the same data. The memory usage with pytorch is also hard to estimate, running vuw-job-report 332320 shows: Nodes: 1 Cores per node: 2 CPU Utilized: 00 :00:07 CPU Efficiency: 43 .75% of 00 :00:16 core-walltime Job Wall-clock time: 00 :00:08 Memory Utilized: 1 .38 MB Memory Efficiency: 0 .00% of 60 .00 GB The memory usage is very low, but there is a very brief spike in memory at the end of the run as the image is generated that vuw-job-report doesn't quite capture. 60G of memory is needed to ensure this completes - a good rule of thumb is to allocate at least as much system memory as GPU memory. The A100's have 40G of ram.","title":"Style some images - inference"},{"location":"training/gpu-neural-style/#train-a-new-style-computationally-expensive","text":"Training a new image style is where we will get the greatest speedup using a GPU. We will use 13G of training images - COCO 2014 Training images dataset . These images have already been downloaded and are accessable at /nfs/home/training/neural_style_data/train2014/ . Note that training a new style will take about 1:15h on an A100 and two and a half hours on an RTX6000 #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=03:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # style-weight and content-weight are just parameters adjusted to give better results python neural_style/neural_style.py train \\ --dataset /nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/style5e10_content_5e4 \\ --style-weight 5e10 \\ --content-weight 5e4 \\ --epochs 2 \\ --cuda 1 This will take a while, but should eventually complete. The A100 has enough memory to train on this image, with other GPUs you may need to scale down the style image to fit in the GPU memory. Note: If you get an out of GPU memory error but it seems the GPU has plenty of memory, it often means you ran out of system memory, try asking for more memory in slurm.","title":"Train a new style - computationally expensive."},{"location":"training/gpu-neural-style/#use-our-newly-trained-network","text":"submit_gpu.sh #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=00:15:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=2 #SBATCH --mem=60G module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU and we'll save the output image as test2.jpg # python neural_style/neural_style.py eval \\ --content-image images/content-images/Octopus_vulgaris_02.JPG \\ --model saved_models/style5e10_content_5e4 \\ --output-image ./test3.jpg --cuda 1","title":"Use our newly trained network"},{"location":"training/gpu-neural-style/#bonus-content-use-a-slurm-task-array-to-find-the-optimum-parameters","text":"In the above example we use parameters for style-weight and content-weight. There are lots of possibilities for these parameters, we can use a task array and a parameter list to determine good values. Note that actually running this example will consume a lot of resources and it is presented mostly to provide some information about task arrays. Running this example will consume the whole GPU partition for about 12 hours. First let's create a list of parameters to test, we could include these in the batch submission script, but I think it's clearer to separate them out. If you're version controlling your submission script, it'll make it easier to see what are changes to parameters and what are changes to the script itself. In the parameter list, the first column is style-weight parameters and the second is content-weight parameters. paramlist.txt 5e10 1e3 5e10 1e4 5e10 5e4 1e11 1e3 1e11 1e4 1e11 5e4 5e11 1e3 5e11 1e4 5e11 5e4 1e12 1e3 1e12 1e4 1e12 5e4 In our submission script we will parse these values with awk . Awk is a bit beyond the scope of this lesson, but it is a handy shell tool for manipulating text. Digital ocean has a nice primer on Awk submit_gpu_train_array #!/bin/bash #SBATCH --job-name=pytorch_test #SBATCH -o _test.out #SBATCH -e _test.err #SBATCH --time=10:00:00 #SBATCH --partition=gpu #SBATCH --gres=gpu:1 #SBATCH --ntasks=10 #SBATCH --mem=60G #SBATCH --array=1-13 module unuse /home/software/tools/modulefiles/ #unuse the old module system module use /home/software/tools/eb_modulefiles/all/Core #use the new module system module load fosscuda/2020b module load PyTorch/1.7.1 module load torchvision/0.8.2-PyTorch-1.7.1 #Optional source env/bin/activate #activate the virtualenv # Run our job --cuda 1 means run on the GPU # #awk -v var=\"$SLURM_ARRAY_TASK_ID\" 'NR == var {print $1}' paramlist.txt style_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $1}' paramlist.txt ) content_weight = $( awk -v var = \" $SLURM_ARRAY_TASK_ID \" 'NR == var {print $2}' paramlist.txt ) echo $style_weight echo $content_weight python neural_style/neural_style.py train \\ --dataset nfs/home/training/neural_style_data/ \\ --style-image images/style-images/wave.jpg \\ --save-model-dir saved_models/test_params2_epoch2/style ${ style_weight } _content ${ content_weight } \\ --style-weight $style_weight \\ --content-weight $content_weight \\ --epochs 2 \\ --cuda 1","title":"Bonus content use a slurm task-array to find the optimum parameters."},{"location":"training/simple-openmpi-singularity-hybrid/","text":"Simple OpenMPI with Singularity using the hybrid approach. \u00b6 The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem-per-cpu=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done","title":"Simple openmpi singularity hybrid"},{"location":"training/simple-openmpi-singularity-hybrid/#simple-openmpi-with-singularity-using-the-hybrid-approach","text":"The hybrid approach is one way of getting OpenMPI working with containers. It requires the OpenMPI version inside the container to match the OpenMPI outside the container (loaded via module loading). First check what openMPI version we have on R\u0101poi. On R\u0101poi switch to our new modules module unuse /home/software/tools/modulefiles # stop using the older modules module use /home/software/tools/eb_modulefiles/all/Core #the new module files organised by compiler module spider OpenMPI # search for openMPI - thre are several options, lets try module spider OpenMPI/4.0.5 # we will use this one, which requires GCC/10.2.0 On your local machine we will create a very simple C openMPI program. Create this in a sensible place. I used ~/projects/examples/singularity/openMPI #include <mpi.h> #include <stdio.h> #include <stdlib.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed\" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed\" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed\" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d\" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } In the same location as above create a singularity definition file, note that we choose to compile and install the same OpenMPI version as we will use on R\u0101poi. Bootstrap: docker From: ubuntu:latest %files mpitest.c /opt %environment export OMPI_DIR = /opt/ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR /bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR /lib %post echo \"Installing required packages...\" apt-get update && apt-get install -y wget git bash gcc gfortran g++ make file echo \"Installing Open MPI\" export OMPI_DIR = /opt/ompi export OMPI_VERSION = 4 .0.5 #NOTE matching version to that on Raapoi export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi- $OMPI_VERSION .tar.bz2\" mkdir -p /tmp/ompi mkdir -p /opt # Download cd /tmp/ompi && wget -O openmpi- $OMPI_VERSION .tar.bz2 $OMPI_URL && tar -xjf openmpi- $OMPI_VERSION .tar.bz2 # Compile and install cd /tmp/ompi/openmpi- $OMPI_VERSION && ./configure --prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR /bin: $PATH export LD_LIBRARY_PATH = $OMPI_DIR /lib: $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR /share/man: $MANPATH echo \"Compiling the MPI application...\" cd /opt && mpicc -o mpitest mpitest.c Now we build our container locally, giving it a sensible name. We need OpenMPI-4.0.5 to use this, so let's include that in the name. sudo singularity build test-openmpi-4.0.5.sif test-openmpi-4.0.5.def Copy that file to R\u0101poi somehow - Filezilla, rsync or similar. I'll just use sftp for simplicity. sftp <username>@raapoi.vuw.ac.nz put test-openmpi-4.0.5.sif Now on R\u0101poi copy that file to a sensible location, I used ~/projects/examples/singularity/openMPI again. mv test-openmpi-4.0.5.sif ~/projects/examples/singularity/openMPI/ cd ~/projects/examples/singularity/openMPI/ In that location create a sbatch file openmpi-test.sh #!/bin/bash #SBATCH --job-name=mpi_test #SBATCH --time=00-00:02:00 #SBATCH --output=out_test.out #SBATCH --error=out_test.err #SBATCH --partition=parallel #SBATCH --ntasks=2 #SBATCH --cpus-per-task=1 #SBATCH --tasks-per-node=1 #SBATCH --mem-per-cpu=1GB #SBATCH --constraint=\"IB,AMD\" #SBATCH --nodes=2 module use /home/software/tools/eb_modulefiles/all/Core module unuse /home/software/tools/modulefiles # to prevent conflicts with the old modules module load GCC/10.2.0 module load OpenMPI/4.0.5 module load Singularity/3.7.3 # Note this is a new singularity build CONPATH = $HOME /projects/examples/singularity/openMPI mpirun -np 2 singularity exec $CONPATH /test-openmpi-4.0.5.sif /opt/mpitest Submit that to slurm and see the output sbatch openmpi-test.sh squeue -u $USER # see the job cat out_test.out # examine the output after the job is done","title":"Simple OpenMPI with Singularity using the hybrid approach."},{"location":"training/simple-tensorflow/","text":"Simple tensorflow example (using new module system) \u00b6 In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible errors - Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc /var/lib/slurm/slurmd/job1125851/slurm_script: line 21: 46983 Aborted (core dumped) python example.py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples In the tensorflow-simple directory","title":"Simple tensorflow"},{"location":"training/simple-tensorflow/#simple-tensorflow-example-using-new-module-system","text":"In a sensible location create an example python script - this is basically copied verbatim from the tensorflow docs: ps://www.tensorflow.org/tutorials/quickstart/beginner example.py import tensorflow as tf print ( \"TensorFlow version:\" , tf . __version__ ) # Load and prepare the MNIST dataset. Convert the sample data from integers to floating-point numbers mnist = tf . keras . datasets . mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () x_train , x_test = x_train / 255.0 , x_test / 255.0 # Build a machine learning model model = tf . keras . models . Sequential ([ tf . keras . layers . Flatten ( input_shape = ( 28 , 28 )), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dropout ( 0.2 ), tf . keras . layers . Dense ( 10 ) ]) # The model returns a vector of log-odds scores, one for each class predictions = model ( x_train [: 1 ]) . numpy () predictions # The tf.nn.softmax function converts these log odds to probabilities for each class tf . nn . softmax ( predictions ) . numpy () # Define a loss function for training. loss_fn = tf . keras . losses . SparseCategoricalCrossentropy ( from_logits = True ) # This untrained model gives probabilities close to random loss_fn ( y_train [: 1 ], predictions ) . numpy () # Configure and compile the model using Keras Model.compile model . compile ( optimizer = 'adam' , loss = loss_fn , metrics = [ 'accuracy' ]) # Train and evaluate the model - use Model.fit to adjust parameters and minimize loss model . fit ( x_train , y_train , epochs = 5 ) # Check model performance model . evaluate ( x_test , y_test , verbose = 2 ) # Return a probability - wrap the trained model and attach softmax probability_model = tf . keras . Sequential ([ model , tf . keras . layers . Softmax () ]) probability_model ( x_test [: 5 ]) Next create a submission script submit.sh #!/bin/bash #SBATCH --job-name=tensoflow_test #SBATCH -o _test.out #SBATCH --time=00:10:00 #SBATCH --partition=gpu #SBATCH --ntasks=6 #SBATCH --mem=50G #SBATCH --gres=gpu:1 # Use the new module system module use /home/software/tools/eb_modulefiles/all/Core #to load tf 2.6.0 you'll first need the compiler set it was built with module load foss/2021a #load tf module load TensorFlow/2.6.0-CUDA-11.3.1 # Run the simple tensorflow example - taken from the docs: https://www.tensorflow.org/tutorials/quickstart/beginner python example.py Submit your job to the queue and then observe in the queue sbatch submit.sh squeue -u <username> Possible errors - Tensorflow jobs on the gpu nodes can be a bit dicey I'd suggest always choosing more memory than the GPU has (40GB) the gpu nodes have a lot of memory so I'd suggest asking for 50GB of ram minimum. There is also a relationship between cpu's allocated and memory used - the errors are not always obvious. If you're running into issues try increasing the requested memory or reducing the requested CPUs Example errors due to requesting many cpus while requesting only 50GB ram Note std::bad_alloc - this suggests a problem allocating memory terminate called after throwing an instance of 'std::bad_alloc' what(): std::bad_alloc /var/lib/slurm/slurmd/job1125851/slurm_script: line 21: 46983 Aborted (core dumped) python example.py Note this example is also in our example git repo: https://github.com/vuw-research-computing/raapoi-examples In the tensorflow-simple directory","title":"Simple tensorflow example (using new module system)"},{"location":"usersub/vpn-alts/","text":"VPN alternatives \u00b6 While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems. ECS ssh bastions \u00b6 ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user> OpenConnect \u00b6 OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi.","title":"Vpn alts"},{"location":"usersub/vpn-alts/#vpn-alternatives","text":"While the official way to connect to R\u0101poi is using the Cisco Anyconnect client, there are some alternatives if that is causing problems.","title":"VPN alternatives"},{"location":"usersub/vpn-alts/#ecs-ssh-bastions","text":"ECS users can come in via the ECS ssh bastions greta-pt.ecs.vuw.ac.nz or barretts.ecs.vuw.ac.nz . Note that this will only work for users with valid ECS credentials. The best way to do this is with a ProxyJump either in the ssh command directly, or you can add it to your ssh_config file. Directly ssh -J <ecs user>@barretts.ecs.vuw.ac.nz <raapoi user>@raapoi.vuw.ac.nz Or via you ssh_config file Host raapoi HostName raapoi.vuw.ac.nz ProxyJump <ECS user>@greta-pt.ecs.vuw.ac.nz User <Raapoi user>","title":"ECS ssh bastions"},{"location":"usersub/vpn-alts/#openconnect","text":"OpenConnect is an opensource implimentation of the Cisco anyconnect clients. Some users find this easier to use than the official client. As all VPN connections currently require 2FA you need to use the openconnect-sso python package . This has only been tested in Linux, it should be possible to make this work in a windows WSL2 terminal as well as in MacOS, but it may require modifications. Step 0: make sure you've already setup the Microsoft Authenticator App on your phone or setup SMS as the 2FA method. sudo apt install pipx pipx install \"openconnect-sso[full]\" # can just use \"openconnect-sso\" if Qt 5.x already installed pipx ensurepath Last line required because of the 'Note' that displays when you run pipx install \"openconnect-sso[full]\" If you have Qt 5.x installed you can skip 2 to 3 and instead: pipx install openconect-sso] Now you can connect using CLI: openconnect-sso --server vpn.vuw.ac.nz --user <firstname.lastname>@vuw.ac.nz It remembers the last used server so after the first time you can reconnect with just openconnect-sso You then just leave a tab of your command line open on this, and in a different tab connect to Raapoi.","title":"OpenConnect"}]}